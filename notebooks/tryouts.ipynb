{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcf7ad65",
   "metadata": {},
   "source": [
    "# Smart Data Quality Agent - Demo Notebook\n",
    "\n",
    "## What This Notebook Does\n",
    "This notebook demonstrates the complete Smart DQ Check workflow with enhanced null analysis:\n",
    "\n",
    "1. **Schema Indexing** - Automatically discovers and indexes database tables (must run for first time)\n",
    "2. **Smart Analysis** - Uses AI to understand your query and analyze the right tables  \n",
    "3. **Enhanced Report Generation** - Creates comprehensive data quality reports with detailed null analysis in multiple formats\n",
    "4. **Fixed Null Analysis Issue**: Enhanced agent prompt to ensure detailed null values breakdown is always included\n",
    "5. **Complete Data Coverage**: Reports now show per-column null counts and percentages\n",
    "6. **Multi-Format Output**: All formats (Markdown, HTML, JSON) contain complete null analysis details\n",
    "\n",
    "## How to Use\n",
    "1. **First Time Setup**: Run cell 4 to build schema index and discover your database tables\n",
    "2. **Run Analysis**: Cell 6 prompts for your query - try examples like:\n",
    "   - \"Generate comprehensive DQ report for prod customers table\"\n",
    "   - \"Create data quality report for production database invoices\"\n",
    "3. **Get Reports**: System generates detailed reports saved to `../reports/` in 3 formats (.html, .json, .md)\n",
    "\n",
    "## Reports being saved under reports folder shows\n",
    "- **Duplicate Record Analysis** - Complete duplicate detection and counts\n",
    "- **Detailed Null Values Assessment** - Per-column breakdown with counts and percentages  \n",
    "- **Statistical Summaries** - Comprehensive data profiling with descriptive stats\n",
    "- **Actionable Recommendations** - Data quality improvement suggestions\n",
    "\n",
    "**Just run all cells in order and follow the prompts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "596a0e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import re\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from src.retrieval.schema_indexer import SchemaIndexer\n",
    "from src.agent.smart_planner import run_smart_dq_check\n",
    "import src.reporting.report_templates\n",
    "import src.data_quality.checks\n",
    "import src.reporting.report_generator\n",
    "import src.agent.reporting_tools\n",
    "import src.reporting.report_processor\n",
    "import src.reporting.report_templates\n",
    "import src.reporting.report_generator\n",
    "from src.reporting.report_processor import SmartDQReportProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df062309",
   "metadata": {},
   "source": [
    "### Schema Indexing - Always rebuild from scratch to ensure fresh data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac1a168d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " BUILDING SCHEMA INDEX FROM SCRATCH\n",
      "\n",
      " Rebuilding index for SNOWFLAKE\n",
      "  Starting fresh - recreating entire index\n",
      " Connecting to Snowflake...\n",
      " Connecting to Snowflake...\n",
      "âœ“ Connected to Snowflake: PROD_SALES.PUBLIC\n",
      "âœ“ Connected to Snowflake: PROD_SALES.PUBLIC\n",
      "âœ“ Disconnected from Snowflake\n",
      "Auto-discovered 4 schemas on Snowflake: DATA_MART_FINANCE, DATA_MART_MANUFACTURING, DATA_MART_MARKETING, PUBLIC\n",
      "[1/4] INDEXING SCHEMA: DATA_MART_FINANCE\n",
      "âœ“ Disconnected from Snowflake\n",
      "Auto-discovered 4 schemas on Snowflake: DATA_MART_FINANCE, DATA_MART_MANUFACTURING, DATA_MART_MARKETING, PUBLIC\n",
      "[1/4] INDEXING SCHEMA: DATA_MART_FINANCE\n",
      "\n",
      "Generating metadata documents for 2 tables...\n",
      "  [1/2] Processing INCOME_BAND...\n",
      "\n",
      "Generating metadata documents for 2 tables...\n",
      "  [1/2] Processing INCOME_BAND...\n",
      "  [2/2] Processing INVOICES...\n",
      "  [2/2] Processing INVOICES...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Deleted existing collection: database_schemas\n",
      "\n",
      "Indexing 2 table metadata documents...\n",
      "Indexed 2 tables from schema DATA_MART_FINANCE\n",
      "[2/4] INDEXING SCHEMA: DATA_MART_MANUFACTURING\n",
      "Indexed 2 tables from schema DATA_MART_FINANCE\n",
      "[2/4] INDEXING SCHEMA: DATA_MART_MANUFACTURING\n",
      "\n",
      "Generating metadata documents for 1 tables...\n",
      "  [1/1] Processing PRODUCTS...\n",
      "\n",
      "Generating metadata documents for 1 tables...\n",
      "  [1/1] Processing PRODUCTS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Indexing 1 table metadata documents...\n",
      "Indexed 1 tables from schema DATA_MART_MANUFACTURING\n",
      "[3/4] INDEXING SCHEMA: DATA_MART_MARKETING\n",
      "\n",
      "Generating metadata documents for 3 tables...\n",
      "  [1/3] Processing CALL_CENTER...\n",
      "\n",
      "Generating metadata documents for 3 tables...\n",
      "  [1/3] Processing CALL_CENTER...\n",
      "  [2/3] Processing CUSTOMERS...\n",
      "  [2/3] Processing CUSTOMERS...\n",
      "  [3/3] Processing SALES...\n",
      "  [3/3] Processing SALES...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Indexing 3 table metadata documents...\n",
      "Indexed 3 tables from schema DATA_MART_MARKETING\n",
      "[4/4] INDEXING SCHEMA: PUBLIC\n",
      "\n",
      "Generating metadata documents for 0 tables...\n",
      "No tables found in schema PUBLIC\n",
      " SCHEMA INDEX BUILT SUCCESSFULLY\n",
      "  - Collection: database_schemas\n",
      "  - Schemas indexed: 4\n",
      "  - Total tables indexed: 6\n",
      "   Index created for snowflake\n",
      "\n",
      " Rebuilding index for POSTGRES\n",
      "  Adding to existing index\n",
      "\n",
      "Generating metadata documents for 0 tables...\n",
      "No tables found in schema PUBLIC\n",
      " SCHEMA INDEX BUILT SUCCESSFULLY\n",
      "  - Collection: database_schemas\n",
      "  - Schemas indexed: 4\n",
      "  - Total tables indexed: 6\n",
      "   Index created for snowflake\n",
      "\n",
      " Rebuilding index for POSTGRES\n",
      "  Adding to existing index\n",
      " Connecting to postgres...\n",
      " Connecting to postgres...\n",
      "âœ“ Connected to PostgreSQL: stage_sales\n",
      "âœ“ Disconnected from PostgreSQL\n",
      "Auto-discovered 1 schemas on postgres: public\n",
      "[1/1] INDEXING SCHEMA: public\n",
      "Discovering tables in stage_sales.public...\n",
      "âœ“ Found 3 tables/views\n",
      "\n",
      "Generating metadata documents for 3 tables...\n",
      "  [1/3] Processing customers...\n",
      "âœ“ Connected to PostgreSQL: stage_sales\n",
      "âœ“ Disconnected from PostgreSQL\n",
      "Auto-discovered 1 schemas on postgres: public\n",
      "[1/1] INDEXING SCHEMA: public\n",
      "Discovering tables in stage_sales.public...\n",
      "âœ“ Found 3 tables/views\n",
      "\n",
      "Generating metadata documents for 3 tables...\n",
      "  [1/3] Processing customers...\n",
      "Discovering tables in stage_sales.public...\n",
      "âœ“ Found 3 tables/views\n",
      "Discovering tables in stage_sales.public...\n",
      "âœ“ Found 3 tables/views\n",
      "  [2/3] Processing invoices...\n",
      "Discovering tables in stage_sales.public...\n",
      "âœ“ Found 3 tables/views\n",
      "  [2/3] Processing invoices...\n",
      "Discovering tables in stage_sales.public...\n",
      "âœ“ Found 3 tables/views\n",
      "  [3/3] Processing products...\n",
      "Discovering tables in stage_sales.public...\n",
      "âœ“ Found 3 tables/views\n",
      "  [3/3] Processing products...\n",
      "Discovering tables in stage_sales.public...\n",
      "âœ“ Found 3 tables/views\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Indexing 3 table metadata documents...\n",
      "Indexed 3 tables from schema public\n",
      " SCHEMA INDEX BUILT SUCCESSFULLY\n",
      "  - Collection: database_schemas\n",
      "  - Schemas indexed: 1\n",
      "  - Total tables indexed: 3\n",
      "   Index updated for postgres\n",
      "\n",
      " SCHEMA INDEX REBUILD COMPLETE!\n"
     ]
    }
   ],
   "source": [
    "# Load settings.yaml to read connector types dynamically\n",
    "with open('../config/settings.yaml', 'r') as f:\n",
    "    settings = yaml.safe_load(f)\n",
    "\n",
    "# Get connector types from settings.yaml\n",
    "connector_types = list(settings['connectors'].keys())\n",
    "\n",
    "print(f\" BUILDING SCHEMA INDEX FROM SCRATCH\")\n",
    "\n",
    "# First connector: recreate=True\n",
    "# Subsequent connectors: recreate=False to preserve other connector data\n",
    "for i, ct in enumerate(connector_types):\n",
    "    print(f\"\\n Rebuilding index for {ct.upper()}\")\n",
    "\n",
    "    # First connector recreates the entire index, others append to it\n",
    "    recreate_index = (i == 0)  # Only recreate on first connector\n",
    "\n",
    "    if recreate_index:\n",
    "        print(f\"  Starting fresh - recreating entire index\")\n",
    "    else:\n",
    "        print(f\"  Adding to existing index\")\n",
    "\n",
    "    indexer = SchemaIndexer(connector_type=ct)\n",
    "    indexer.build_schema_index(recreate=recreate_index)\n",
    "    print(f\"   Index {'created' if recreate_index else 'updated'} for {ct}\")\n",
    "\n",
    "print(f\"\\n SCHEMA INDEX REBUILD COMPLETE!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bb9e9e",
   "metadata": {},
   "source": [
    "## Enhanced Comprehensive Report Creation\n",
    "\n",
    "Generate comprehensive data quality reports with detailed duplicate counts, null analysis, and descriptive statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c65981e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using query: comprehensive report staging database customers table\n",
      "\n",
      "======================================================================\n",
      "SMART DATA QUALITY CHECK\n",
      "======================================================================\n",
      "User Query: comprehensive report staging database customers table\n",
      "\n",
      "Step 1: Discovering relevant tables...\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Number of requested results 20 is greater than number of elements in index 9, updating n_results = 9\n",
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Number of requested results 20 is greater than number of elements in index 9, updating n_results = 9\n",
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 1 relevant table(s) (â‰¥5% relevance):\n",
      "\n",
      "  1. [POSTGRES] stage_sales.public.customers\n",
      "     Relevance: 5.17% âœ“\n",
      "\n",
      "Step 2: Building context for agent...\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Step 3: Running agent with discovered tables...\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `generate_comprehensive_dq_report` with `{'dataset_id': 'stage_sales.public.customers', 'connector_type': 'postgres', 'output_format': 'markdown'}`\n",
      "\n",
      "\n",
      "\u001b[0mRunning duplicates check...\n",
      "--- Loading data for: stage_sales.public.customers using POSTGRES connector ---\n",
      "âœ“ Connected to PostgreSQL: stage_sales\n",
      "Executing query: SELECT * FROM stage_sales.public.customers\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `generate_comprehensive_dq_report` with `{'dataset_id': 'stage_sales.public.customers', 'connector_type': 'postgres', 'output_format': 'markdown'}`\n",
      "\n",
      "\n",
      "\u001b[0mRunning duplicates check...\n",
      "--- Loading data for: stage_sales.public.customers using POSTGRES connector ---\n",
      "âœ“ Connected to PostgreSQL: stage_sales\n",
      "Executing query: SELECT * FROM stage_sales.public.customers\n",
      "âœ“ Loaded 50000 rows from PostgreSQL\n",
      "âœ“ Disconnected from PostgreSQL\n",
      "Running null_values check...\n",
      "--- Loading data for: stage_sales.public.customers using POSTGRES connector ---\n",
      "âœ“ Loaded 50000 rows from PostgreSQL\n",
      "âœ“ Disconnected from PostgreSQL\n",
      "Running null_values check...\n",
      "--- Loading data for: stage_sales.public.customers using POSTGRES connector ---\n",
      "âœ“ Connected to PostgreSQL: stage_sales\n",
      "Executing query: SELECT * FROM stage_sales.public.customers\n",
      "âœ“ Connected to PostgreSQL: stage_sales\n",
      "Executing query: SELECT * FROM stage_sales.public.customers\n",
      "âœ“ Loaded 50000 rows from PostgreSQL\n",
      "âœ“ Disconnected from PostgreSQL\n",
      "âœ“ Loaded 50000 rows from PostgreSQL\n",
      "âœ“ Disconnected from PostgreSQL\n",
      "Running descriptive_stats check...\n",
      "--- Loading data for: stage_sales.public.customers using POSTGRES connector ---\n",
      "âœ“ Connected to PostgreSQL: stage_sales\n",
      "Executing query: SELECT * FROM stage_sales.public.customers\n",
      "Running descriptive_stats check...\n",
      "--- Loading data for: stage_sales.public.customers using POSTGRES connector ---\n",
      "âœ“ Connected to PostgreSQL: stage_sales\n",
      "Executing query: SELECT * FROM stage_sales.public.customers\n",
      "âœ“ Loaded 50000 rows from PostgreSQL\n",
      "âœ“ Disconnected from PostgreSQL\n",
      "âœ“ Converted 'customer_id' to categorical for descriptive statistics\n",
      "âœ“ Loaded 50000 rows from PostgreSQL\n",
      "âœ“ Disconnected from PostgreSQL\n",
      "âœ“ Converted 'customer_id' to categorical for descriptive statistics\n",
      "\u001b[33;1m\u001b[1;3m{'dataset_id': 'stage_sales.public.customers', 'connector_type': 'postgres', 'report_format': 'markdown', 'report_content': '# Data Quality Assessment Report\\n\\n## Dataset Information\\n- **Dataset**: `stage_sales.public.customers`\\n- **Connector**: POSTGRES\\n- **Assessment Date**: 2025-11-26 00:45:14\\n- **Total Checks**: 3\\n\\n## Executive Summary\\n- âœ… **Passed**: 1 checks\\n- âŒ **Failed**: 2 checks\\n- ðŸš« **Errors**: 0 checks\\n\\n---\\n\\n## Detailed Check Results\\n\\n### âŒ Duplicates\\n\\n- **Status**: ERROR\\n- **Error**: Unknown error\\n\\n### âœ… Null Values\\n\\n- **Status**: FAIL\\n- **Dataset Size**: 50,000 rows Ã— 22 columns\\n- **Columns with Missing Data**: 4 out of 22 columns\\n- **Missing Data Coverage**: 18.2% of columns affected\\n\\n**Detailed Null Analysis:**\\n- `company`: 9,000 nulls (18.0%)\\n- `phone`: 7,500 nulls (15.0%)\\n- `email`: 6,000 nulls (12.0%)\\n- `address`: 4,000 nulls (8.0%)\\n\\n### âœ… Descriptive Stats\\n\\n- **Status**: PASS (Statistics Generated)\\\\n- **Columns Analyzed**: 22 columns\\\\n\\\\n**ðŸ“Š Complete Statistical Summary:**\\\\n\\\\n| Column | Count | Mean | Std | Min | 25% | 50% | 75% | Max | Unique | Top | Freq |\\\\n|--------|-------|------|-----|-----|-----|-----|-----|-----|--------|-----|------|\\\\n| `customer_id` | 50,000.00 | - | - | - | - | - | - | - | 46,000.00 | 15,036.00 | 4.00 |\\\\n| `first_name` | 50,000 | - | - | - | - | - | - | - | 690 | Michael | 1,102 |\\\\n| `last_name` | 50,000 | - | - | - | - | - | - | - | 1000 | Smith | 1,137 |\\\\n| `email` | 44,000 | - | - | - | - | - | - | - | 37995 | pwilliams@examp | 10 |\\\\n| `phone` | 42,500 | - | - | - | - | - | - | - | 39581 | 611.462.7724 | 4 |\\\\n| `address` | 46,000 | - | - | - | - | - | - | - | 42615 | 029 George Plai | 4 |\\\\n| `city` | 50,000 | - | - | - | - | - | - | - | 23860 | West Michael | 54 |\\\\n| `state` | 50,000 | - | - | - | - | - | - | - | 50 | Nevada | 1,075 |\\\\n| `zip_code` | 50,000 | - | - | - | - | - | - | - | 36805 | 20762 | 6 |\\\\n| `country` | 50,000 | - | - | - | - | - | - | - | 243 | Korea | 440 |\\\\n| `date_of_birth` | 50,000 | - | - | - | - | - | - | - | 20933 | 1964-12-02 | 13 |\\\\n| `age` | 50,000.00 | 77.60 | 127.63 | 18.00 | 35.00 | 53.00 | 71.00 | 998.00 | - | - | - |\\\\n| `gender` | 50,000 | - | - | - | - | - | - | - | 3 | M | 16,701 |\\\\n| `company` | 41,000 | - | - | - | - | - | - | - | 29125 | Smith Inc | 59 |\\\\n| `job_title` | 50,000 | - | - | - | - | - | - | - | 639 | Commercial art  | 108 |\\\\n| `income` | 50,000.00 | 108,792.65 | 52,212.69 | -49,970.00 | 75,629.75 | 111,537.50 | 144,938.00 | 255,659.00 | - | - | - |\\\\n| `account_balance` | 50,000.00 | 30,021,883.97 | 170,585,079.08 | -4,997.18 | 9,329.11 | 23,390.89 | 37,626.61 | 999,999,999.99 | - | - | - |\\\\n| `credit_score` | 50,000.00 | 575.00 | 159.05 | 300.00 | 437.00 | 574.00 | 713.00 | 850.00 | - | - | - |\\\\n| `registration_date` | 50,000 | - | - | - | - | - | - | - | 1827 | 2024-09-13 | 47 |\\\\n| `last_login` | 50,000 | - | - | - | - | - | - | - | 90 | 2025-10-09 | 625 |\\\\n| `is_active` | 50,000 | - | - | - | - | - | - | - | 2 | False | 25,048 |\\\\n| `customer_segment` | 50,000 | - | - | - | - | - | - | - | 4 | Standard | 12,691 |\\\\n\\n---\\n\\n## Recommendations\\n\\n1. **Establish Data Quality Framework**\\n   - Only 33% of checks passed. Consider implementing: 1) Data validation at ingestion, 2) Regular quality monitoring, 3) Data stewardship roles, 4) Quality metrics dashboards.\\n   - *Priority*: High\\n\\n2. **Implement Null Handling Strategy**\\n   - Columns `company`, `phone`, `email` have 5-20% null values. Consider implementing NOT NULL constraints, data validation rules, or imputation strategies based on business logic.\\n   - *Priority*: Medium\\n\\n3. **Investigate High Variability in account_balance**\\n   - Column `account_balance` has high variability (CV=5.68). Review for outliers, data entry errors, or consider data transformation/normalization techniques.\\n   - *Priority*: Low\\n\\n4. **Data Documentation and Profiling**\\n   - Dataset has 22 columns. Consider creating data dictionaries, establishing column-level monitoring, and implementing regular statistical profiling to track data drift over time.\\n   - *Priority*: Low\\n\\n---\\n\\n## Report Generation\\n- **Generated by**: LLM Data Quality Agent\\n- **Generation Time**: 2025-11-26 00:45:14\\n- **Report Format**: Markdown\\n\\n*This report was automatically generated. Please review recommendations and take appropriate action.*\\n', 'summary': {'total_checks': 3, 'passed_checks': 1, 'failed_checks': 2, 'error_checks': 0, 'pass_rate': '33.3%'}, 'recommendations_count': 4, 'status': 'success'}\u001b[0m\u001b[33;1m\u001b[1;3m{'dataset_id': 'stage_sales.public.customers', 'connector_type': 'postgres', 'report_format': 'markdown', 'report_content': '# Data Quality Assessment Report\\n\\n## Dataset Information\\n- **Dataset**: `stage_sales.public.customers`\\n- **Connector**: POSTGRES\\n- **Assessment Date**: 2025-11-26 00:45:14\\n- **Total Checks**: 3\\n\\n## Executive Summary\\n- âœ… **Passed**: 1 checks\\n- âŒ **Failed**: 2 checks\\n- ðŸš« **Errors**: 0 checks\\n\\n---\\n\\n## Detailed Check Results\\n\\n### âŒ Duplicates\\n\\n- **Status**: ERROR\\n- **Error**: Unknown error\\n\\n### âœ… Null Values\\n\\n- **Status**: FAIL\\n- **Dataset Size**: 50,000 rows Ã— 22 columns\\n- **Columns with Missing Data**: 4 out of 22 columns\\n- **Missing Data Coverage**: 18.2% of columns affected\\n\\n**Detailed Null Analysis:**\\n- `company`: 9,000 nulls (18.0%)\\n- `phone`: 7,500 nulls (15.0%)\\n- `email`: 6,000 nulls (12.0%)\\n- `address`: 4,000 nulls (8.0%)\\n\\n### âœ… Descriptive Stats\\n\\n- **Status**: PASS (Statistics Generated)\\\\n- **Columns Analyzed**: 22 columns\\\\n\\\\n**ðŸ“Š Complete Statistical Summary:**\\\\n\\\\n| Column | Count | Mean | Std | Min | 25% | 50% | 75% | Max | Unique | Top | Freq |\\\\n|--------|-------|------|-----|-----|-----|-----|-----|-----|--------|-----|------|\\\\n| `customer_id` | 50,000.00 | - | - | - | - | - | - | - | 46,000.00 | 15,036.00 | 4.00 |\\\\n| `first_name` | 50,000 | - | - | - | - | - | - | - | 690 | Michael | 1,102 |\\\\n| `last_name` | 50,000 | - | - | - | - | - | - | - | 1000 | Smith | 1,137 |\\\\n| `email` | 44,000 | - | - | - | - | - | - | - | 37995 | pwilliams@examp | 10 |\\\\n| `phone` | 42,500 | - | - | - | - | - | - | - | 39581 | 611.462.7724 | 4 |\\\\n| `address` | 46,000 | - | - | - | - | - | - | - | 42615 | 029 George Plai | 4 |\\\\n| `city` | 50,000 | - | - | - | - | - | - | - | 23860 | West Michael | 54 |\\\\n| `state` | 50,000 | - | - | - | - | - | - | - | 50 | Nevada | 1,075 |\\\\n| `zip_code` | 50,000 | - | - | - | - | - | - | - | 36805 | 20762 | 6 |\\\\n| `country` | 50,000 | - | - | - | - | - | - | - | 243 | Korea | 440 |\\\\n| `date_of_birth` | 50,000 | - | - | - | - | - | - | - | 20933 | 1964-12-02 | 13 |\\\\n| `age` | 50,000.00 | 77.60 | 127.63 | 18.00 | 35.00 | 53.00 | 71.00 | 998.00 | - | - | - |\\\\n| `gender` | 50,000 | - | - | - | - | - | - | - | 3 | M | 16,701 |\\\\n| `company` | 41,000 | - | - | - | - | - | - | - | 29125 | Smith Inc | 59 |\\\\n| `job_title` | 50,000 | - | - | - | - | - | - | - | 639 | Commercial art  | 108 |\\\\n| `income` | 50,000.00 | 108,792.65 | 52,212.69 | -49,970.00 | 75,629.75 | 111,537.50 | 144,938.00 | 255,659.00 | - | - | - |\\\\n| `account_balance` | 50,000.00 | 30,021,883.97 | 170,585,079.08 | -4,997.18 | 9,329.11 | 23,390.89 | 37,626.61 | 999,999,999.99 | - | - | - |\\\\n| `credit_score` | 50,000.00 | 575.00 | 159.05 | 300.00 | 437.00 | 574.00 | 713.00 | 850.00 | - | - | - |\\\\n| `registration_date` | 50,000 | - | - | - | - | - | - | - | 1827 | 2024-09-13 | 47 |\\\\n| `last_login` | 50,000 | - | - | - | - | - | - | - | 90 | 2025-10-09 | 625 |\\\\n| `is_active` | 50,000 | - | - | - | - | - | - | - | 2 | False | 25,048 |\\\\n| `customer_segment` | 50,000 | - | - | - | - | - | - | - | 4 | Standard | 12,691 |\\\\n\\n---\\n\\n## Recommendations\\n\\n1. **Establish Data Quality Framework**\\n   - Only 33% of checks passed. Consider implementing: 1) Data validation at ingestion, 2) Regular quality monitoring, 3) Data stewardship roles, 4) Quality metrics dashboards.\\n   - *Priority*: High\\n\\n2. **Implement Null Handling Strategy**\\n   - Columns `company`, `phone`, `email` have 5-20% null values. Consider implementing NOT NULL constraints, data validation rules, or imputation strategies based on business logic.\\n   - *Priority*: Medium\\n\\n3. **Investigate High Variability in account_balance**\\n   - Column `account_balance` has high variability (CV=5.68). Review for outliers, data entry errors, or consider data transformation/normalization techniques.\\n   - *Priority*: Low\\n\\n4. **Data Documentation and Profiling**\\n   - Dataset has 22 columns. Consider creating data dictionaries, establishing column-level monitoring, and implementing regular statistical profiling to track data drift over time.\\n   - *Priority*: Low\\n\\n---\\n\\n## Report Generation\\n- **Generated by**: LLM Data Quality Agent\\n- **Generation Time**: 2025-11-26 00:45:14\\n- **Report Format**: Markdown\\n\\n*This report was automatically generated. Please review recommendations and take appropriate action.*\\n', 'summary': {'total_checks': 3, 'passed_checks': 1, 'failed_checks': 2, 'error_checks': 0, 'pass_rate': '33.3%'}, 'recommendations_count': 4, 'status': 'success'}\u001b[0m\u001b[32;1m\u001b[1;3m# Comprehensive Data Quality Assessment Report\n",
      "\n",
      "## Dataset Information\n",
      "- **Dataset**: `stage_sales.public.customers`\n",
      "- **Connector**: POSTGRES\n",
      "- **Assessment Date**: 2025-11-26\n",
      "- **Total Checks**: 3\n",
      "\n",
      "---\n",
      "\n",
      "## Executive Summary\n",
      "- âœ… **Passed**: 1 checks\n",
      "- âŒ **Failed**: 2 checks\n",
      "- ðŸš« **Errors**: 0 checks\n",
      "\n",
      "---\n",
      "\n",
      "## Detailed Check Results\n",
      "\n",
      "### âŒ Duplicates\n",
      "- **Status**: ERROR\n",
      "- **Error**: Unknown error\n",
      "\n",
      "---\n",
      "\n",
      "### âœ… Null Values\n",
      "- **Status**: FAIL\n",
      "- **Dataset Size**: 50,000 rows Ã— 22 columns\n",
      "- **Columns with Missing Data**: 4 out of 22 columns\n",
      "- **Missing Data Coverage**: 18.2% of columns affected\n",
      "\n",
      "**Detailed Null Analysis**:\n",
      "- `company`: 9,000 nulls (18.0%)\n",
      "- `phone`: 7,500 nulls (15.0%)\n",
      "- `email`: 6,000 nulls (12.0%)\n",
      "- `address`: 4,000 nulls (8.0%)\n",
      "\n",
      "---\n",
      "\n",
      "### âœ… Descriptive Stats\n",
      "- **Status**: PASS (Statistics Generated)\n",
      "- **Columns Analyzed**: 22 columns\n",
      "\n",
      "**ðŸ“Š Complete Statistical Summary**:\n",
      "\n",
      "| Column              | Count     | Mean         | Std          | Min       | 25%       | 50%       | 75%       | Max          | Unique   | Top                  | Freq |\n",
      "|---------------------|-----------|--------------|--------------|-----------|-----------|-----------|-----------|--------------|----------|----------------------|------|\n",
      "| `customer_id`       | 50,000.00 | -            | -            | -         | -         | -         | -         | -            | 46,000.00 | 15,036.00           | 4    |\n",
      "| `first_name`        | 50,000    | -            | -            | -         | -         | -         | -         | -            | 690      | Michael             | 1,102|\n",
      "| `last_name`         | 50,000    | -            | -            | -         | -         | -         | -         | -            | 1,000    | Smith               | 1,137|\n",
      "| `email`             | 44,000    | -            | -            | -         | -         | -         | -         | -            | 37,995   | pwilliams@examp     | 10   |\n",
      "| `phone`             | 42,500    | -            | -            | -         | -         | -         | -         | -            | 39,581   | 611.462.7724        | 4    |\n",
      "| `address`           | 46,000    | -            | -            | -         | -         | -         | -         | -            | 42,615   | 029 George Plai     | 4    |\n",
      "| `city`              | 50,000    | -            | -            | -         | -         | -         | -         | -            | 23,860   | West Michael        | 54   |\n",
      "| `state`             | 50,000    | -            | -            | -         | -         | -         | -         | -            | 50       | Nevada              | 1,075|\n",
      "| `zip_code`          | 50,000    | -            | -            | -         | -         | -         | -         | -            | 36,805   | 20762              | 6    |\n",
      "| `country`           | 50,000    | -            | -            | -         | -         | -         | -         | -            | 243      | Korea               | 440  |\n",
      "| `date_of_birth`     | 50,000    | -            | -            | -         | -         | -         | -         | -            | 20,933   | 1964-12-02         | 13   |\n",
      "| `age`               | 50,000.00 | 77.60        | 127.63       | 18.00     | 35.00     | 53.00     | 71.00     | 998.00       | -        | -                  | -    |\n",
      "| `gender`            | 50,000    | -            | -            | -         | -         | -         | -         | -            | 3        | M                  | 16,701|\n",
      "| `company`           | 41,000    | -            | -            | -         | -         | -         | -         | -            | 29,125   | Smith Inc           | 59   |\n",
      "| `job_title`         | 50,000    | -            | -            | -         | -         | -         | -         | -            | 639      | Commercial art      | 108  |\n",
      "| `income`            | 50,000.00 | 108,792.65   | 52,212.69    | -49,970.00| 75,629.75 | 111,537.50| 144,938.00| 255,659.00   | -        | -                  | -    |\n",
      "| `account_balance`   | 50,000.00 | 30,021,883.97| 170,585,079.08| -4,997.18 | 9,329.11  | 23,390.89 | 37,626.61 | 999,999,999.99| -        | -                  | -    |\n",
      "| `credit_score`      | 50,000.00 | 575.00       | 159.05       | 300.00    | 437.00    | 574.00    | 713.00    | 850.00       | -        | -                  | -    |\n",
      "| `registration_date` | 50,000    | -            | -            | -         | -         | -         | -         | -            | 1,827    | 2024-09-13         | 47   |\n",
      "| `last_login`        | 50,000    | -            | -            | -         | -         | -         | -         | -            | 90       | 2025-10-09         | 625  |\n",
      "| `is_active`         | 50,000    | -            | -            | -         | -         | -         | -         | -            | 2        | False              | 25,048|\n",
      "| `customer_segment`  | 50,000    | -            | -            | -         | -         | -         | -         | -            | 4        | Standard            | 12,691|\n",
      "\n",
      "---\n",
      "\n",
      "## Recommendations\n",
      "\n",
      "1. **Establish Data Quality Framework**\n",
      "   - Only 33% of checks passed. Consider implementing: 1) Data validation at ingestion, 2) Regular quality monitoring, 3) Data stewardship roles, 4) Quality metrics dashboards.\n",
      "   - *Priority*: High\n",
      "\n",
      "2. **Implement Null Handling Strategy**\n",
      "   - Columns `company`, `phone`, `email` have 5-20% null values. Consider implementing NOT NULL constraints, data validation rules, or imputation strategies based on business logic.\n",
      "   - *Priority*: Medium\n",
      "\n",
      "3. **Investigate High Variability in account_balance**\n",
      "   - Column `account_balance` has high variability (CV=5.68). Review for outliers, data entry errors, or consider data transformation/normalization techniques.\n",
      "   - *Priority*: Low\n",
      "\n",
      "4. **Data Documentation and Profiling**\n",
      "   - Dataset has 22 columns. Consider creating data dictionaries, establishing column-level monitoring, and implementing regular statistical profiling to track data drift over time.\n",
      "   - *Priority*: Low\n",
      "\n",
      "---\n",
      "\n",
      "## Report Generation\n",
      "- **Generated by**: LLM Data Quality Agent\n",
      "- **Generation Time**: 2025-11-26\n",
      "- **Report Format**: Markdown\n",
      "\n",
      "*This report was automatically generated. Please review recommendations and take appropriate action.*\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "======================================================================\n",
      "RESULT\n",
      "======================================================================\n",
      "\n",
      "\u001b[32;1m\u001b[1;3m# Comprehensive Data Quality Assessment Report\n",
      "\n",
      "## Dataset Information\n",
      "- **Dataset**: `stage_sales.public.customers`\n",
      "- **Connector**: POSTGRES\n",
      "- **Assessment Date**: 2025-11-26\n",
      "- **Total Checks**: 3\n",
      "\n",
      "---\n",
      "\n",
      "## Executive Summary\n",
      "- âœ… **Passed**: 1 checks\n",
      "- âŒ **Failed**: 2 checks\n",
      "- ðŸš« **Errors**: 0 checks\n",
      "\n",
      "---\n",
      "\n",
      "## Detailed Check Results\n",
      "\n",
      "### âŒ Duplicates\n",
      "- **Status**: ERROR\n",
      "- **Error**: Unknown error\n",
      "\n",
      "---\n",
      "\n",
      "### âœ… Null Values\n",
      "- **Status**: FAIL\n",
      "- **Dataset Size**: 50,000 rows Ã— 22 columns\n",
      "- **Columns with Missing Data**: 4 out of 22 columns\n",
      "- **Missing Data Coverage**: 18.2% of columns affected\n",
      "\n",
      "**Detailed Null Analysis**:\n",
      "- `company`: 9,000 nulls (18.0%)\n",
      "- `phone`: 7,500 nulls (15.0%)\n",
      "- `email`: 6,000 nulls (12.0%)\n",
      "- `address`: 4,000 nulls (8.0%)\n",
      "\n",
      "---\n",
      "\n",
      "### âœ… Descriptive Stats\n",
      "- **Status**: PASS (Statistics Generated)\n",
      "- **Columns Analyzed**: 22 columns\n",
      "\n",
      "**ðŸ“Š Complete Statistical Summary**:\n",
      "\n",
      "| Column              | Count     | Mean         | Std          | Min       | 25%       | 50%       | 75%       | Max          | Unique   | Top                  | Freq |\n",
      "|---------------------|-----------|--------------|--------------|-----------|-----------|-----------|-----------|--------------|----------|----------------------|------|\n",
      "| `customer_id`       | 50,000.00 | -            | -            | -         | -         | -         | -         | -            | 46,000.00 | 15,036.00           | 4    |\n",
      "| `first_name`        | 50,000    | -            | -            | -         | -         | -         | -         | -            | 690      | Michael             | 1,102|\n",
      "| `last_name`         | 50,000    | -            | -            | -         | -         | -         | -         | -            | 1,000    | Smith               | 1,137|\n",
      "| `email`             | 44,000    | -            | -            | -         | -         | -         | -         | -            | 37,995   | pwilliams@examp     | 10   |\n",
      "| `phone`             | 42,500    | -            | -            | -         | -         | -         | -         | -            | 39,581   | 611.462.7724        | 4    |\n",
      "| `address`           | 46,000    | -            | -            | -         | -         | -         | -         | -            | 42,615   | 029 George Plai     | 4    |\n",
      "| `city`              | 50,000    | -            | -            | -         | -         | -         | -         | -            | 23,860   | West Michael        | 54   |\n",
      "| `state`             | 50,000    | -            | -            | -         | -         | -         | -         | -            | 50       | Nevada              | 1,075|\n",
      "| `zip_code`          | 50,000    | -            | -            | -         | -         | -         | -         | -            | 36,805   | 20762              | 6    |\n",
      "| `country`           | 50,000    | -            | -            | -         | -         | -         | -         | -            | 243      | Korea               | 440  |\n",
      "| `date_of_birth`     | 50,000    | -            | -            | -         | -         | -         | -         | -            | 20,933   | 1964-12-02         | 13   |\n",
      "| `age`               | 50,000.00 | 77.60        | 127.63       | 18.00     | 35.00     | 53.00     | 71.00     | 998.00       | -        | -                  | -    |\n",
      "| `gender`            | 50,000    | -            | -            | -         | -         | -         | -         | -            | 3        | M                  | 16,701|\n",
      "| `company`           | 41,000    | -            | -            | -         | -         | -         | -         | -            | 29,125   | Smith Inc           | 59   |\n",
      "| `job_title`         | 50,000    | -            | -            | -         | -         | -         | -         | -            | 639      | Commercial art      | 108  |\n",
      "| `income`            | 50,000.00 | 108,792.65   | 52,212.69    | -49,970.00| 75,629.75 | 111,537.50| 144,938.00| 255,659.00   | -        | -                  | -    |\n",
      "| `account_balance`   | 50,000.00 | 30,021,883.97| 170,585,079.08| -4,997.18 | 9,329.11  | 23,390.89 | 37,626.61 | 999,999,999.99| -        | -                  | -    |\n",
      "| `credit_score`      | 50,000.00 | 575.00       | 159.05       | 300.00    | 437.00    | 574.00    | 713.00    | 850.00       | -        | -                  | -    |\n",
      "| `registration_date` | 50,000    | -            | -            | -         | -         | -         | -         | -            | 1,827    | 2024-09-13         | 47   |\n",
      "| `last_login`        | 50,000    | -            | -            | -         | -         | -         | -         | -            | 90       | 2025-10-09         | 625  |\n",
      "| `is_active`         | 50,000    | -            | -            | -         | -         | -         | -         | -            | 2        | False              | 25,048|\n",
      "| `customer_segment`  | 50,000    | -            | -            | -         | -         | -         | -         | -            | 4        | Standard            | 12,691|\n",
      "\n",
      "---\n",
      "\n",
      "## Recommendations\n",
      "\n",
      "1. **Establish Data Quality Framework**\n",
      "   - Only 33% of checks passed. Consider implementing: 1) Data validation at ingestion, 2) Regular quality monitoring, 3) Data stewardship roles, 4) Quality metrics dashboards.\n",
      "   - *Priority*: High\n",
      "\n",
      "2. **Implement Null Handling Strategy**\n",
      "   - Columns `company`, `phone`, `email` have 5-20% null values. Consider implementing NOT NULL constraints, data validation rules, or imputation strategies based on business logic.\n",
      "   - *Priority*: Medium\n",
      "\n",
      "3. **Investigate High Variability in account_balance**\n",
      "   - Column `account_balance` has high variability (CV=5.68). Review for outliers, data entry errors, or consider data transformation/normalization techniques.\n",
      "   - *Priority*: Low\n",
      "\n",
      "4. **Data Documentation and Profiling**\n",
      "   - Dataset has 22 columns. Consider creating data dictionaries, establishing column-level monitoring, and implementing regular statistical profiling to track data drift over time.\n",
      "   - *Priority*: Low\n",
      "\n",
      "---\n",
      "\n",
      "## Report Generation\n",
      "- **Generated by**: LLM Data Quality Agent\n",
      "- **Generation Time**: 2025-11-26\n",
      "- **Report Format**: Markdown\n",
      "\n",
      "*This report was automatically generated. Please review recommendations and take appropriate action.*\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "======================================================================\n",
      "RESULT\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get user input for the query with a default option\n",
    "default_query = \"Generate comprehensive DQ report for prod customers table\"\n",
    "user_query = input(\"Enter your DQ check query (or press Enter to use default): \").strip() or default_query\n",
    "print(f\"Using query: {user_query}\")\n",
    "\n",
    "comprehensive_report = run_smart_dq_check(user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f053675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smart DQ check successful - proceeding with report generation...\n",
      "Extracted info:\n",
      "   Dataset: stage_sales.public.customers\n",
      "   Connector: postgres\n",
      "\n",
      "Running comprehensive DQ assessment with reliable data sources...\n",
      "   Executing duplicates check...\n",
      "--- Loading data for: stage_sales.public.customers using POSTGRES connector ---\n",
      "âœ“ Connected to PostgreSQL: stage_sales\n",
      "Executing query: SELECT * FROM stage_sales.public.customers\n",
      "âœ“ Loaded 50000 rows from PostgreSQL\n",
      "âœ“ Disconnected from PostgreSQL\n",
      "   Executing null_values check...\n",
      "--- Loading data for: stage_sales.public.customers using POSTGRES connector ---\n",
      "âœ“ Loaded 50000 rows from PostgreSQL\n",
      "âœ“ Disconnected from PostgreSQL\n",
      "   Executing null_values check...\n",
      "--- Loading data for: stage_sales.public.customers using POSTGRES connector ---\n",
      "âœ“ Connected to PostgreSQL: stage_sales\n",
      "Executing query: SELECT * FROM stage_sales.public.customers\n",
      "âœ“ Connected to PostgreSQL: stage_sales\n",
      "Executing query: SELECT * FROM stage_sales.public.customers\n",
      "âœ“ Loaded 50000 rows from PostgreSQL\n",
      "âœ“ Disconnected from PostgreSQL\n",
      "âœ“ Loaded 50000 rows from PostgreSQL\n",
      "âœ“ Disconnected from PostgreSQL\n",
      "   Executing descriptive_stats check...\n",
      "--- Loading data for: stage_sales.public.customers using POSTGRES connector ---\n",
      "âœ“ Connected to PostgreSQL: stage_sales\n",
      "Executing query: SELECT * FROM stage_sales.public.customers\n",
      "   Executing descriptive_stats check...\n",
      "--- Loading data for: stage_sales.public.customers using POSTGRES connector ---\n",
      "âœ“ Connected to PostgreSQL: stage_sales\n",
      "Executing query: SELECT * FROM stage_sales.public.customers\n",
      "âœ“ Loaded 50000 rows from PostgreSQL\n",
      "âœ“ Disconnected from PostgreSQL\n",
      "âœ“ Converted 'customer_id' to categorical for descriptive statistics\n",
      "   âœ… Assessment completed using direct DQ check execution!\n",
      "   âœ… Data consistency guaranteed - no parsing errors!\n",
      "\n",
      "Generating reports using standardized templates...\n",
      "   MARKDOWN: ../reports/comprehensive_dq_report_stage_sales_customers_20251126_004534.md\n",
      "   HTML: ../reports/comprehensive_dq_report_stage_sales_customers_20251126_004534.html\n",
      "   JSON: ../reports/comprehensive_dq_report_stage_sales_customers_20251126_004534.json\n",
      "âœ“ Loaded 50000 rows from PostgreSQL\n",
      "âœ“ Disconnected from PostgreSQL\n",
      "âœ“ Converted 'customer_id' to categorical for descriptive statistics\n",
      "   âœ… Assessment completed using direct DQ check execution!\n",
      "   âœ… Data consistency guaranteed - no parsing errors!\n",
      "\n",
      "Generating reports using standardized templates...\n",
      "   MARKDOWN: ../reports/comprehensive_dq_report_stage_sales_customers_20251126_004534.md\n",
      "   HTML: ../reports/comprehensive_dq_report_stage_sales_customers_20251126_004534.html\n",
      "   JSON: ../reports/comprehensive_dq_report_stage_sales_customers_20251126_004534.json\n"
     ]
    }
   ],
   "source": [
    "# Initialize the report processor\n",
    "processor = SmartDQReportProcessor(reports_dir=\"../reports\")\n",
    "\n",
    "# Process the comprehensive report (handles validation, extraction, checks, and file generation)\n",
    "result = processor.process_comprehensive_report(comprehensive_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-dq-agent-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
