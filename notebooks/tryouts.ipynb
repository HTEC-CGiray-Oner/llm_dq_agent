{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcf7ad65",
   "metadata": {},
   "source": [
    "## Demo notebook showcase entire process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596a0e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import re\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from src.retrieval.schema_indexer import SchemaIndexer\n",
    "from src.agent.smart_planner import run_smart_dq_check\n",
    "import src.reporting.report_templates\n",
    "import src.data_quality.checks\n",
    "import src.reporting.report_generator\n",
    "import src.agent.reporting_tools\n",
    "from src.reporting.report_templates import ReportTemplates\n",
    "from src.reporting.report_generator import DataQualityReportGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df062309",
   "metadata": {},
   "source": [
    "### Indexing target schemas and tables based on samples data and table metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1a168d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building schema index for connector type: snowflake\n",
      " Connecting to Snowflake...\n",
      " Connecting to Snowflake...\n",
      "‚úì Connected to Snowflake: PROD_SALES.PUBLIC\n",
      "‚úì Connected to Snowflake: PROD_SALES.PUBLIC\n",
      "‚úì Disconnected from Snowflake\n",
      "Auto-discovered 4 schemas on Snowflake: DATA_MART_FINANCE, DATA_MART_MANUFACTURING, DATA_MART_MARKETING, PUBLIC\n",
      "[1/4] INDEXING SCHEMA: DATA_MART_FINANCE\n",
      "‚úì Disconnected from Snowflake\n",
      "Auto-discovered 4 schemas on Snowflake: DATA_MART_FINANCE, DATA_MART_MANUFACTURING, DATA_MART_MARKETING, PUBLIC\n",
      "[1/4] INDEXING SCHEMA: DATA_MART_FINANCE\n",
      "\n",
      "Generating metadata documents for 1 tables...\n",
      "  [1/1] Processing INVOICES...\n",
      "\n",
      "Generating metadata documents for 1 tables...\n",
      "  [1/1] Processing INVOICES...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Deleted existing collection: database_schemas\n",
      "\n",
      "Indexing 1 table metadata documents...\n",
      "Indexed 1 tables from schema DATA_MART_FINANCE\n",
      "[2/4] INDEXING SCHEMA: DATA_MART_MANUFACTURING\n",
      "\n",
      "Generating metadata documents for 1 tables...\n",
      "  [1/1] Processing PRODUCTS...\n",
      "\n",
      "Generating metadata documents for 1 tables...\n",
      "  [1/1] Processing PRODUCTS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Indexing 1 table metadata documents...\n",
      "Indexed 1 tables from schema DATA_MART_MANUFACTURING\n",
      "[3/4] INDEXING SCHEMA: DATA_MART_MARKETING\n",
      "\n",
      "Generating metadata documents for 1 tables...\n",
      "  [1/1] Processing CUSTOMERS...\n",
      "\n",
      "Generating metadata documents for 1 tables...\n",
      "  [1/1] Processing CUSTOMERS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Indexing 1 table metadata documents...\n",
      "Indexed 1 tables from schema DATA_MART_MARKETING\n",
      "[4/4] INDEXING SCHEMA: PUBLIC\n",
      "\n",
      "Generating metadata documents for 0 tables...\n",
      "No tables found in schema PUBLIC\n",
      " SCHEMA INDEX BUILT SUCCESSFULLY\n",
      "  - Collection: database_schemas\n",
      "  - Schemas indexed: 4\n",
      "  - Total tables indexed: 3\n",
      "Building schema index for connector type: postgres\n",
      "\n",
      "Generating metadata documents for 0 tables...\n",
      "No tables found in schema PUBLIC\n",
      " SCHEMA INDEX BUILT SUCCESSFULLY\n",
      "  - Collection: database_schemas\n",
      "  - Schemas indexed: 4\n",
      "  - Total tables indexed: 3\n",
      "Building schema index for connector type: postgres\n",
      " Connecting to postgres...\n",
      "‚úì Connected to PostgreSQL: stage_sales\n",
      "‚úì Disconnected from PostgreSQL\n",
      "Auto-discovered 1 schemas on postgres: public\n",
      "[1/1] INDEXING SCHEMA: public\n",
      " Connecting to postgres...\n",
      "‚úì Connected to PostgreSQL: stage_sales\n",
      "‚úì Disconnected from PostgreSQL\n",
      "Auto-discovered 1 schemas on postgres: public\n",
      "[1/1] INDEXING SCHEMA: public\n",
      "Discovering tables in stage_sales.public...\n",
      "‚úì Found 3 tables/views\n",
      "\n",
      "Generating metadata documents for 3 tables...\n",
      "  [1/3] Processing customers...\n",
      "Discovering tables in stage_sales.public...\n",
      "‚úì Found 3 tables/views\n",
      "Discovering tables in stage_sales.public...\n",
      "‚úì Found 3 tables/views\n",
      "\n",
      "Generating metadata documents for 3 tables...\n",
      "  [1/3] Processing customers...\n",
      "Discovering tables in stage_sales.public...\n",
      "‚úì Found 3 tables/views\n",
      "  [2/3] Processing invoices...\n",
      "Discovering tables in stage_sales.public...\n",
      "‚úì Found 3 tables/views\n",
      "  [2/3] Processing invoices...\n",
      "Discovering tables in stage_sales.public...\n",
      "‚úì Found 3 tables/views\n",
      "  [3/3] Processing products...\n",
      "Discovering tables in stage_sales.public...\n",
      "‚úì Found 3 tables/views\n",
      "  [3/3] Processing products...\n",
      "Discovering tables in stage_sales.public...\n",
      "‚úì Found 3 tables/views\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Indexing 3 table metadata documents...\n",
      "Indexed 3 tables from schema public\n",
      " SCHEMA INDEX BUILT SUCCESSFULLY\n",
      "  - Collection: database_schemas\n",
      "  - Schemas indexed: 1\n",
      "  - Total tables indexed: 3\n"
     ]
    }
   ],
   "source": [
    "# Load settings.yaml to read connector types dynamically\n",
    "with open('../config/settings.yaml', 'r') as f:\n",
    "    settings = yaml.safe_load(f)\n",
    "\n",
    "# Get connector types from settings.yaml\n",
    "connector_types = list(settings['connectors'].keys())\n",
    "\n",
    "for ct in connector_types:\n",
    "    print(f\"Building schema index for connector type: {ct}\")\n",
    "    # recreate=False to avoid rebuilding entire index, created ones for second run recreate=False right choice\n",
    "    if ct == 'snowflake':\n",
    "        SchemaIndexer(connector_type=ct).build_schema_index(recreate=True)\n",
    "    else:\n",
    "        SchemaIndexer(connector_type=ct).build_schema_index(recreate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bb9e9e",
   "metadata": {},
   "source": [
    "## Enhanced Comprehensive Report Creation\n",
    "\n",
    "Generate comprehensive data quality reports with detailed duplicate counts, null analysis, and descriptive statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65981e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Running smart DQ check with relevance threshold protection...\n",
      "\n",
      "======================================================================\n",
      "SMART DATA QUALITY CHECK\n",
      "======================================================================\n",
      "User Query: generate comprehensive report for me unknown table\n",
      "\n",
      "Step 1: Discovering relevant tables...\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Number of requested results 20 is greater than number of elements in index 6, updating n_results = 6\n",
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Number of requested results 20 is greater than number of elements in index 6, updating n_results = 6\n",
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Smart DQ Check Response:\n",
      "--------------------------------------------------\n",
      "No relevant tables found. Please ensure the schema index is built. Run: python src/retrieval/schema_indexer.py\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "comprehensive_report = run_smart_dq_check(\n",
    "    \"Generate comprehensive DQ report for prod customers table\"\n",
    "    #  \"generate comprehensive report for me unknown table\"\n",
    ")\n",
    "\n",
    "print(\"\\n Smart DQ Check Response:\")\n",
    "print(comprehensive_report.get('output', 'No output returned'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f053675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Report content preview: No relevant tables found. Please ensure the schema index is built. Run: python src/retrieval/schema_indexer.py...\n",
      "‚úÖ Smart DQ check successful - proceeding with report generation...\n",
      "‚ö†Ô∏è Could not extract dataset from successful report - this shouldn't happen!\n"
     ]
    }
   ],
   "source": [
    "# Check if the smart DQ check was successful before proceeding with report generation\n",
    "report_output = comprehensive_report.get('output', '')\n",
    "print(f\"Report content preview: {report_output[:200]}...\")\n",
    "\n",
    "# Check for relevance threshold failure\n",
    "if \"No tables found with sufficient relevance\" in report_output:\n",
    "    print(\"Smart DQ check failed due to low relevance scores.\")\n",
    "    print(\"Skipping report generation - no valid table found for analysis.\")\n",
    "    print(\"Try using a more specific table name in your query.\")\n",
    "else:\n",
    "    print(\"Smart DQ check successful - proceeding with report generation...\")\n",
    "\n",
    "    # Create reports directory one folder up\n",
    "    reports_dir = \"../reports\"\n",
    "    os.makedirs(reports_dir, exist_ok=True)\n",
    "\n",
    "    # Initialize report generator\n",
    "    generator = DataQualityReportGenerator()\n",
    "\n",
    "    # Generate timestamp for unique filenames\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    # Extract dataset and connector info from the comprehensive_report\n",
    "    dataset_id = None\n",
    "    connector_type = None\n",
    "\n",
    "    # Look for patterns like \"stage_sales.public.customers\" or \"PROD_SALES.PUBLIC.CUSTOMERS\"\n",
    "    dataset_patterns = [\n",
    "        r'`([^`]+\\.public\\.[^`]+)`',  # Pattern: `schema.public.table`\n",
    "        r'([A-Z_]+\\.[A-Z_]+\\.[A-Z_]+)',  # Pattern: SCHEMA.PUBLIC.TABLE\n",
    "        r'([a-z_]+\\.public\\.[a-z_]+)',   # Pattern: schema.public.table\n",
    "    ]\n",
    "\n",
    "    for pattern in dataset_patterns:\n",
    "        match = re.search(pattern, report_output)\n",
    "        if match:\n",
    "            dataset_id = match.group(1)\n",
    "            break\n",
    "\n",
    "    # Determine connector type based on dataset naming convention or explicit mention\n",
    "    if dataset_id:\n",
    "        if 'PROD_SALES' in dataset_id.upper() or 'snowflake' in report_output.lower():\n",
    "            connector_type = 'snowflake'\n",
    "        elif 'STAGE_SALES' in dataset_id.upper() or 'postgres' in report_output.lower():\n",
    "            connector_type = 'postgres'\n",
    "        else:\n",
    "            # Default based on case - uppercase typically Snowflake, lowercase typically Postgres\n",
    "            connector_type = 'snowflake' if dataset_id.isupper() else 'postgres'\n",
    "\n",
    "    # Only proceed if we successfully extracted dataset info\n",
    "    if not dataset_id:\n",
    "        print(\"Could not extract dataset from successful report - this shouldn't happen!\")\n",
    "    else:\n",
    "        print(f\"Extracted info:\")\n",
    "        print(f\"   Dataset: {dataset_id}\")\n",
    "        print(f\"   Connector: {connector_type}\")\n",
    "\n",
    "        base_filename = f\"comprehensive_dq_report_{timestamp}\"\n",
    "\n",
    "        print(f\"\\nRunning optimized DQ assessment...\")\n",
    "\n",
    "        # Use the optimized approach: run DQ checks directly and create assessment\n",
    "        from src.data_quality.checks import check_dataset_duplicates, check_dataset_null_values, check_dataset_descriptive_stats\n",
    "\n",
    "        # Run all DQ checks directly\n",
    "        print(\"   Running duplicate check...\")\n",
    "        duplicate_results = check_dataset_duplicates(dataset_id, connector_type=connector_type)\n",
    "\n",
    "        print(\"   Running null values check...\")\n",
    "        null_results = check_dataset_null_values(dataset_id, connector_type=connector_type)\n",
    "\n",
    "        print(\"   Running descriptive statistics...\")\n",
    "        stats_results = check_dataset_descriptive_stats(dataset_id, connector_type=connector_type)\n",
    "\n",
    "        # Create assessment using the optimized method (correct parameter name)\n",
    "        check_results = {\n",
    "            'duplicates': duplicate_results,\n",
    "            'null_values': null_results,\n",
    "            'descriptive_stats': stats_results\n",
    "        }\n",
    "\n",
    "        assessment_results = generator.create_assessment_from_results(\n",
    "            check_results=check_results,\n",
    "            dataset_id=dataset_id,\n",
    "            connector_type=connector_type\n",
    "        )\n",
    "\n",
    "        # Display summary of actual results\n",
    "        print(f\"\\nData Quality Results for {dataset_id}:\")\n",
    "        print(f\"   Passed: {assessment_results['summary']['passed_checks']} checks\")\n",
    "        print(f\"   Failed: {assessment_results['summary']['failed_checks']} checks\")\n",
    "        print(f\"   Errors: {assessment_results['summary']['error_checks']} checks\")\n",
    "\n",
    "        print(f\"\\nGenerating reports using standardized templates...\")\n",
    "\n",
    "        # 1. Save as Markdown (.md) using template\n",
    "        md_file = f\"{reports_dir}/{base_filename}.md\"\n",
    "        md_content = generator.generate_markdown_report(assessment_results)\n",
    "        with open(md_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(md_content)\n",
    "        print(f\"   MARKDOWN: {md_file}\")\n",
    "\n",
    "        # 2. Save as HTML (.html) using template\n",
    "        html_file = f\"{reports_dir}/{base_filename}.html\"\n",
    "        html_content = generator.generate_html_report(assessment_results)\n",
    "        with open(html_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(html_content)\n",
    "        print(f\"   HTML: {html_file}\")\n",
    "\n",
    "        # 3. Save as JSON (.json) using generator\n",
    "        json_file = f\"{reports_dir}/{base_filename}.json\"\n",
    "        json_content = generator.generate_json_report(assessment_results)\n",
    "        with open(json_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(json_content)\n",
    "        print(f\"   JSON: {json_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-dq-agent-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
