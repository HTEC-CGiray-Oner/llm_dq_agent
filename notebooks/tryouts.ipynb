{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcf7ad65",
   "metadata": {},
   "source": [
    "# Setup: Build Schema Index\n",
    "\n",
    "First, we need to index the table schemas from our databases so the agent can discover them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "596a0e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\CihanGirayOner\\Desktop\\git_repos\\llm_dq_agent\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Connected to Snowflake: PROD_SALES.PUBLIC\n",
      "✓ Disconnected from Snowflake\n",
      "Auto-discovered 3 schemas: DATA_MART_MANUFACTURING, DATA_MART_MARKETING, PUBLIC\n",
      "\n",
      "======================================================================\n",
      "[1/3] INDEXING SCHEMA: DATA_MART_MANUFACTURING\n",
      "======================================================================\n",
      "✓ Connected to Snowflake: PROD_SALES.PUBLIC\n",
      "Discovering tables in PROD_SALES.DATA_MART_MANUFACTURING...\n",
      "✓ Found 1 tables/views\n",
      "✓ Disconnected from Snowflake\n",
      "\n",
      "Generating metadata documents for 1 tables...\n",
      "  [1/1] Processing PRODUCTS...\n",
      "✓ Connected to Snowflake: PROD_SALES.PUBLIC\n",
      "Discovering tables in PROD_SALES.DATA_MART_MANUFACTURING...\n",
      "✓ Found 1 tables/views\n",
      "✓ Disconnected from Snowflake\n",
      "✓ Connected to Snowflake: PROD_SALES.PUBLIC\n",
      "✓ Disconnected from Snowflake\n",
      "✓ Connected to Snowflake: PROD_SALES.PUBLIC\n",
      "✓ Disconnected from Snowflake\n",
      "✓ Generated 1 metadata documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Deleted existing collection: database_schemas\n",
      "\n",
      "Indexing 1 table metadata documents...\n",
      "✓ Indexed 1 tables from schema DATA_MART_MANUFACTURING\n",
      "\n",
      "======================================================================\n",
      "[2/3] INDEXING SCHEMA: DATA_MART_MARKETING\n",
      "======================================================================\n",
      "✓ Connected to Snowflake: PROD_SALES.PUBLIC\n",
      "Discovering tables in PROD_SALES.DATA_MART_MARKETING...\n",
      "✓ Found 1 tables/views\n",
      "✓ Disconnected from Snowflake\n",
      "\n",
      "Generating metadata documents for 1 tables...\n",
      "  [1/1] Processing CUSTOMERS...\n",
      "✓ Connected to Snowflake: PROD_SALES.PUBLIC\n",
      "Discovering tables in PROD_SALES.DATA_MART_MARKETING...\n",
      "✓ Found 1 tables/views\n",
      "✓ Disconnected from Snowflake\n",
      "✓ Connected to Snowflake: PROD_SALES.PUBLIC\n",
      "✓ Disconnected from Snowflake\n",
      "✓ Connected to Snowflake: PROD_SALES.PUBLIC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Disconnected from Snowflake\n",
      "✓ Generated 1 metadata documents\n",
      "\n",
      "Indexing 1 table metadata documents...\n",
      "✓ Indexed 1 tables from schema DATA_MART_MARKETING\n",
      "\n",
      "======================================================================\n",
      "[3/3] INDEXING SCHEMA: PUBLIC\n",
      "======================================================================\n",
      "✓ Connected to Snowflake: PROD_SALES.PUBLIC\n",
      "Discovering tables in PROD_SALES.PUBLIC...\n",
      "✓ Found 0 tables/views\n",
      "✓ Disconnected from Snowflake\n",
      "\n",
      "Generating metadata documents for 0 tables...\n",
      "✓ Generated 0 metadata documents\n",
      "No tables found in schema PUBLIC\n",
      "\n",
      "======================================================================\n",
      "✓ SCHEMA INDEX BUILT SUCCESSFULLY\n",
      "  - Collection: database_schemas\n",
      "  - Schemas indexed: 3\n",
      "  - Total tables indexed: 2\n",
      "  - Path: ./chroma_db\n",
      "======================================================================\n",
      "\n",
      "✓ Connected to PostgreSQL: stage_sales\n",
      "✓ Disconnected from PostgreSQL\n",
      "Auto-discovered 1 schemas: public\n",
      "\n",
      "======================================================================\n",
      "[1/1] INDEXING SCHEMA: public\n",
      "======================================================================\n",
      "✓ Connected to PostgreSQL: stage_sales\n",
      "Discovering tables in stage_sales.public...\n",
      "✓ Found 2 tables/views\n",
      "✓ Disconnected from PostgreSQL\n",
      "\n",
      "Generating metadata documents for 2 tables...\n",
      "  [1/2] Processing customers...\n",
      "✓ Connected to PostgreSQL: stage_sales\n",
      "Discovering tables in stage_sales.public...\n",
      "✓ Found 2 tables/views\n",
      "✓ Disconnected from PostgreSQL\n",
      "✓ Connected to PostgreSQL: stage_sales\n",
      "✓ Disconnected from PostgreSQL\n",
      "✓ Connected to PostgreSQL: stage_sales\n",
      "✓ Disconnected from PostgreSQL\n",
      "  [2/2] Processing products...\n",
      "✓ Connected to PostgreSQL: stage_sales\n",
      "Discovering tables in stage_sales.public...\n",
      "✓ Found 2 tables/views\n",
      "✓ Disconnected from PostgreSQL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Connected to PostgreSQL: stage_sales\n",
      "✓ Disconnected from PostgreSQL\n",
      "✓ Connected to PostgreSQL: stage_sales\n",
      "✓ Disconnected from PostgreSQL\n",
      "✓ Generated 2 metadata documents\n",
      "\n",
      "Indexing 2 table metadata documents...\n",
      "✓ Indexed 2 tables from schema public\n",
      "\n",
      "======================================================================\n",
      "✓ SCHEMA INDEX BUILT SUCCESSFULLY\n",
      "  - Collection: database_schemas\n",
      "  - Schemas indexed: 1\n",
      "  - Total tables indexed: 2\n",
      "  - Path: ./chroma_db\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src.retrieval.schema_indexer import SchemaIndexer\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Index Snowflake first (recreate the collection)\n",
    "snowflake_indexer = SchemaIndexer(connector_type='snowflake')\n",
    "snowflake_indexer.build_schema_index(recreate=True)\n",
    "\n",
    "# Index PostgreSQL second (append to collection)\n",
    "postgres_indexer = SchemaIndexer(connector_type='postgres')\n",
    "postgres_indexer.build_schema_index(recreate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6ba125a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SMART DATA QUALITY CHECK\n",
      "======================================================================\n",
      "User Query: Check for duplicates in PROD_SALES CUSTOMERS table\n",
      "\n",
      "Step 1: Discovering relevant tables...\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event CollectionGetEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Number of requested results 6 is greater than number of elements in index 4, updating n_results = 4\n",
      "Failed to send telemetry event CollectionQueryEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 3 relevant table(s):\n",
      "\n",
      "  1. [SNOWFLAKE] PROD_SALES.DATA_MART_MARKETING.CUSTOMERS\n",
      "     Relevance: 38.13%\n",
      "\n",
      "  2. [POSTGRES] stage_sales.public.customers\n",
      "     Relevance: 0.72%\n",
      "\n",
      "  3. [SNOWFLAKE] PROD_SALES.DATA_MART_MANUFACTURING.PRODUCTS\n",
      "     Relevance: -6.99%\n",
      "\n",
      "Step 2: Building context for agent...\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Step 3: Running agent with discovered tables...\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `check_dataset_duplicates` with `{'dataset_id': 'PROD_SALES.DATA_MART_MARKETING.CUSTOMERS', 'connector_type': 'snowflake'}`\n",
      "\n",
      "\n",
      "\u001b[0m--- Loading data for: PROD_SALES.DATA_MART_MARKETING.CUSTOMERS using SNOWFLAKE connector ---\n",
      "✓ Connected to Snowflake: PROD_SALES.PUBLIC\n",
      "Executing query: SELECT * FROM PROD_SALES.DATA_MART_MARKETING.CUSTOMERS\n",
      "✓ Loaded 37811 rows from Snowflake\n",
      "✓ Disconnected from Snowflake\n",
      "\u001b[36;1m\u001b[1;3m{'dataset_id': 'PROD_SALES.DATA_MART_MARKETING.CUSTOMERS', 'duplicate_qty': 0, 'status': 'success'}\u001b[0m\u001b[32;1m\u001b[1;3mThe duplicate check for the table `PROD_SALES.DATA_MART_MARKETING.CUSTOMERS` in Snowflake has been completed successfully. No duplicate rows were found in the dataset.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "======================================================================\n",
      "RESULT\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "SMART DATA QUALITY CHECK\n",
      "======================================================================\n",
      "User Query: Check for duplicates in STAGE_SALES CUSTOMERS table\n",
      "\n",
      "Step 1: Discovering relevant tables...\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Number of requested results 6 is greater than number of elements in index 4, updating n_results = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 3 relevant table(s):\n",
      "\n",
      "  1. [POSTGRES] stage_sales.public.customers\n",
      "     Relevance: 25.33%\n",
      "\n",
      "  2. [SNOWFLAKE] PROD_SALES.DATA_MART_MARKETING.CUSTOMERS\n",
      "     Relevance: -2.03%\n",
      "\n",
      "  3. [POSTGRES] stage_sales.public.products\n",
      "     Relevance: -11.56%\n",
      "\n",
      "Step 2: Building context for agent...\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Step 3: Running agent with discovered tables...\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `check_dataset_duplicates` with `{'dataset_id': 'stage_sales.public.customers', 'connector_type': 'postgres'}`\n",
      "\n",
      "\n",
      "\u001b[0m--- Loading data for: stage_sales.public.customers using POSTGRES connector ---\n",
      "✓ Connected to PostgreSQL: stage_sales\n",
      "Executing query: SELECT * FROM stage_sales.public.customers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CihanGirayOner\\Desktop\\git_repos\\llm_dq_agent\\src\\connectors\\postgres_connector.py:78: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(sql_query, self._connection)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded 50000 rows from PostgreSQL\n",
      "✓ Disconnected from PostgreSQL\n",
      "\u001b[36;1m\u001b[1;3m{'dataset_id': 'stage_sales.public.customers', 'duplicate_qty': 1134, 'status': 'failure'}\u001b[0m\u001b[32;1m\u001b[1;3mThe duplicate check for the table `stage_sales.public.customers` in PostgreSQL has been completed. There are **1134 duplicate rows** detected in the dataset.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "======================================================================\n",
      "RESULT\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src.agent.smart_planner import run_smart_dq_check\n",
    "\n",
    "result_1 = run_smart_dq_check(\"Check for duplicates in PROD_SALES CUSTOMERS table\")\n",
    "\n",
    "result_2 = run_smart_dq_check(\"Check for duplicates in STAGE_SALES CUSTOMERS table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "412a9e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SMART DATA QUALITY CHECK\n",
      "======================================================================\n",
      "User Query: Check for null values in postgres CUSTOMERS table\n",
      "\n",
      "Step 1: Discovering relevant tables...\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Number of requested results 6 is greater than number of elements in index 4, updating n_results = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 3 relevant table(s):\n",
      "\n",
      "  1. [POSTGRES] stage_sales.public.customers\n",
      "     Relevance: 41.81%\n",
      "\n",
      "  2. [SNOWFLAKE] PROD_SALES.DATA_MART_MARKETING.CUSTOMERS\n",
      "     Relevance: 0.89%\n",
      "\n",
      "  3. [POSTGRES] stage_sales.public.products\n",
      "     Relevance: -14.07%\n",
      "\n",
      "Step 2: Building context for agent...\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Step 3: Running agent with discovered tables...\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `check_dataset_null_values` with `{'dataset_id': 'stage_sales.public.customers', 'connector_type': 'postgres'}`\n",
      "\n",
      "\n",
      "\u001b[0m--- Loading data for: stage_sales.public.customers using POSTGRES connector ---\n",
      "✓ Connected to PostgreSQL: stage_sales\n",
      "Executing query: SELECT * FROM stage_sales.public.customers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CihanGirayOner\\Desktop\\git_repos\\llm_dq_agent\\src\\connectors\\postgres_connector.py:78: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(sql_query, self._connection)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded 50000 rows from PostgreSQL\n",
      "✓ Disconnected from PostgreSQL\n",
      "\u001b[33;1m\u001b[1;3m{'dataset_id': 'stage_sales.public.customers', 'total_rows': 50000, 'total_columns': 22, 'columns_with_nulls': 4, 'null_analysis': [{'column_name': 'company', 'null_count': 9000, 'null_percentage': 18.0}, {'column_name': 'phone', 'null_count': 7500, 'null_percentage': 15.0}, {'column_name': 'email', 'null_count': 6000, 'null_percentage': 12.0}, {'column_name': 'address', 'null_count': 4000, 'null_percentage': 8.0}], 'status': 'success'}\u001b[0m\u001b[32;1m\u001b[1;3mThe null values analysis for the `stage_sales.public.customers` table in PostgreSQL is as follows:\n",
      "\n",
      "- **Total Rows:** 50,000\n",
      "- **Total Columns:** 22\n",
      "- **Columns with Null Values:** 4\n",
      "\n",
      "### Null Analysis:\n",
      "1. **Company**\n",
      "   - Null Count: 9,000\n",
      "   - Null Percentage: 18.0%\n",
      "2. **Phone**\n",
      "   - Null Count: 7,500\n",
      "   - Null Percentage: 15.0%\n",
      "3. **Email**\n",
      "   - Null Count: 6,000\n",
      "   - Null Percentage: 12.0%\n",
      "4. **Address**\n",
      "   - Null Count: 4,000\n",
      "   - Null Percentage: 8.0%\n",
      "\n",
      "Let me know if you need further assistance!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "======================================================================\n",
      "RESULT\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "SMART DATA QUALITY CHECK\n",
      "======================================================================\n",
      "User Query: Analyze missing data in PROD_SALES CUSTOMERS table\n",
      "\n",
      "Step 1: Discovering relevant tables...\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Number of requested results 6 is greater than number of elements in index 4, updating n_results = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 3 relevant table(s):\n",
      "\n",
      "  1. [SNOWFLAKE] PROD_SALES.DATA_MART_MARKETING.CUSTOMERS\n",
      "     Relevance: 43.33%\n",
      "\n",
      "  2. [SNOWFLAKE] PROD_SALES.DATA_MART_MANUFACTURING.PRODUCTS\n",
      "     Relevance: 10.52%\n",
      "\n",
      "  3. [POSTGRES] stage_sales.public.customers\n",
      "     Relevance: -1.66%\n",
      "\n",
      "Step 2: Building context for agent...\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Step 3: Running agent with discovered tables...\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `check_dataset_null_values` with `{'dataset_id': 'PROD_SALES.DATA_MART_MARKETING.CUSTOMERS', 'connector_type': 'snowflake'}`\n",
      "\n",
      "\n",
      "\u001b[0m--- Loading data for: PROD_SALES.DATA_MART_MARKETING.CUSTOMERS using SNOWFLAKE connector ---\n",
      "✓ Connected to Snowflake: PROD_SALES.PUBLIC\n",
      "Executing query: SELECT * FROM PROD_SALES.DATA_MART_MARKETING.CUSTOMERS\n",
      "✓ Loaded 37811 rows from Snowflake\n",
      "✓ Disconnected from Snowflake\n",
      "\u001b[33;1m\u001b[1;3m{'dataset_id': 'PROD_SALES.DATA_MART_MARKETING.CUSTOMERS', 'total_rows': 37811, 'total_columns': 22, 'columns_with_nulls': 3, 'null_analysis': [{'column_name': 'COMPANY', 'null_count': 6927, 'null_percentage': 18.32}, {'column_name': 'PHONE', 'null_count': 5794, 'null_percentage': 15.32}, {'column_name': 'ADDRESS', 'null_count': 3130, 'null_percentage': 8.28}], 'status': 'success'}\u001b[0m\u001b[32;1m\u001b[1;3mThe missing data analysis for the table `PROD_SALES.DATA_MART_MARKETING.CUSTOMERS` in Snowflake has been completed. Here are the results:\n",
      "\n",
      "- **Total Rows:** 37,811\n",
      "- **Total Columns:** 22\n",
      "- **Columns with Null Values:** 3\n",
      "\n",
      "### Null Analysis:\n",
      "1. **COMPANY**\n",
      "   - Null Count: 6,927\n",
      "   - Null Percentage: 18.32%\n",
      "2. **PHONE**\n",
      "   - Null Count: 5,794\n",
      "   - Null Percentage: 15.32%\n",
      "3. **ADDRESS**\n",
      "   - Null Count: 3,130\n",
      "   - Null Percentage: 8.28%\n",
      "\n",
      "Let me know if you need further insights or actions on this data!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "======================================================================\n",
      "RESULT\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "SMART DATA QUALITY CHECK\n",
      "======================================================================\n",
      "User Query: Analyze missing data in STAGE_SALES CUSTOMERS table\n",
      "\n",
      "Step 1: Discovering relevant tables...\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Number of requested results 6 is greater than number of elements in index 4, updating n_results = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 3 relevant table(s):\n",
      "\n",
      "  1. [POSTGRES] stage_sales.public.customers\n",
      "     Relevance: 19.02%\n",
      "\n",
      "  2. [POSTGRES] stage_sales.public.products\n",
      "     Relevance: 2.19%\n",
      "\n",
      "  3. [SNOWFLAKE] PROD_SALES.DATA_MART_MARKETING.CUSTOMERS\n",
      "     Relevance: -0.25%\n",
      "\n",
      "Step 2: Building context for agent...\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Step 3: Running agent with discovered tables...\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `check_dataset_null_values` with `{'dataset_id': 'stage_sales.public.customers', 'connector_type': 'postgres'}`\n",
      "\n",
      "\n",
      "\u001b[0m--- Loading data for: stage_sales.public.customers using POSTGRES connector ---\n",
      "✓ Connected to PostgreSQL: stage_sales\n",
      "Executing query: SELECT * FROM stage_sales.public.customers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CihanGirayOner\\Desktop\\git_repos\\llm_dq_agent\\src\\connectors\\postgres_connector.py:78: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(sql_query, self._connection)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded 50000 rows from PostgreSQL\n",
      "✓ Disconnected from PostgreSQL\n",
      "\u001b[33;1m\u001b[1;3m{'dataset_id': 'stage_sales.public.customers', 'total_rows': 50000, 'total_columns': 22, 'columns_with_nulls': 4, 'null_analysis': [{'column_name': 'company', 'null_count': 9000, 'null_percentage': 18.0}, {'column_name': 'phone', 'null_count': 7500, 'null_percentage': 15.0}, {'column_name': 'email', 'null_count': 6000, 'null_percentage': 12.0}, {'column_name': 'address', 'null_count': 4000, 'null_percentage': 8.0}], 'status': 'success'}\u001b[0m\u001b[32;1m\u001b[1;3mThe analysis of missing data in the `stage_sales.public.customers` table (PostgreSQL) is complete. Here are the results:\n",
      "\n",
      "- **Total Rows:** 50,000\n",
      "- **Total Columns:** 22\n",
      "- **Columns with Null Values:** 4\n",
      "\n",
      "### Null Analysis:\n",
      "1. **Company**\n",
      "   - Null Count: 9,000\n",
      "   - Null Percentage: 18.0%\n",
      "2. **Phone**\n",
      "   - Null Count: 7,500\n",
      "   - Null Percentage: 15.0%\n",
      "3. **Email**\n",
      "   - Null Count: 6,000\n",
      "   - Null Percentage: 12.0%\n",
      "4. **Address**\n",
      "   - Null Count: 4,000\n",
      "   - Null Percentage: 8.0%\n",
      "\n",
      "Let me know if you need further assistance or additional checks!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "======================================================================\n",
      "RESULT\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the new null values check function\n",
    "result_null_1 = run_smart_dq_check(\"Check for null values in postgres CUSTOMERS table\")\n",
    "result_null_2 = run_smart_dq_check(\"Analyze missing data in PROD_SALES CUSTOMERS table\")\n",
    "result_null_3 = run_smart_dq_check(\"Analyze missing data in STAGE_SALES CUSTOMERS table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5a0e801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SMART DATA QUALITY CHECK\n",
      "======================================================================\n",
      "User Query: Analyze missing data And duplicate records in PROD CUSTOMERS table\n",
      "\n",
      "Step 1: Discovering relevant tables...\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Number of requested results 6 is greater than number of elements in index 4, updating n_results = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 3 relevant table(s):\n",
      "\n",
      "  1. [SNOWFLAKE] PROD_SALES.DATA_MART_MARKETING.CUSTOMERS\n",
      "     Relevance: 37.78%\n",
      "\n",
      "  2. [SNOWFLAKE] PROD_SALES.DATA_MART_MANUFACTURING.PRODUCTS\n",
      "     Relevance: 20.51%\n",
      "\n",
      "  3. [POSTGRES] stage_sales.public.customers\n",
      "     Relevance: -3.69%\n",
      "\n",
      "Step 2: Building context for agent...\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Step 3: Running agent with discovered tables...\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `check_dataset_null_values` with `{'dataset_id': 'PROD_SALES.DATA_MART_MARKETING.CUSTOMERS', 'connector_type': 'snowflake'}`\n",
      "\n",
      "\n",
      "\u001b[0m--- Loading data for: PROD_SALES.DATA_MART_MARKETING.CUSTOMERS using SNOWFLAKE connector ---\n",
      "✓ Connected to Snowflake: PROD_SALES.PUBLIC\n",
      "Executing query: SELECT * FROM PROD_SALES.DATA_MART_MARKETING.CUSTOMERS\n",
      "✓ Loaded 37811 rows from Snowflake\n",
      "✓ Disconnected from Snowflake\n",
      "\u001b[33;1m\u001b[1;3m{'dataset_id': 'PROD_SALES.DATA_MART_MARKETING.CUSTOMERS', 'total_rows': 37811, 'total_columns': 22, 'columns_with_nulls': 3, 'null_analysis': [{'column_name': 'COMPANY', 'null_count': 6927, 'null_percentage': 18.32}, {'column_name': 'PHONE', 'null_count': 5794, 'null_percentage': 15.32}, {'column_name': 'ADDRESS', 'null_count': 3130, 'null_percentage': 8.28}], 'status': 'success'}\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `check_dataset_duplicates` with `{'dataset_id': 'PROD_SALES.DATA_MART_MARKETING.CUSTOMERS', 'connector_type': 'snowflake'}`\n",
      "\n",
      "\n",
      "\u001b[0m--- Loading data for: PROD_SALES.DATA_MART_MARKETING.CUSTOMERS using SNOWFLAKE connector ---\n",
      "✓ Connected to Snowflake: PROD_SALES.PUBLIC\n",
      "Executing query: SELECT * FROM PROD_SALES.DATA_MART_MARKETING.CUSTOMERS\n",
      "✓ Loaded 37811 rows from Snowflake\n",
      "✓ Disconnected from Snowflake\n",
      "\u001b[36;1m\u001b[1;3m{'dataset_id': 'PROD_SALES.DATA_MART_MARKETING.CUSTOMERS', 'duplicate_qty': 0, 'status': 'success'}\u001b[0m\u001b[32;1m\u001b[1;3mThe analysis for the `PROD_SALES.DATA_MART_MARKETING.CUSTOMERS` table in Snowflake is complete:\n",
      "\n",
      "### Missing Data Analysis:\n",
      "- **Total Rows:** 37,811\n",
      "- **Total Columns:** 22\n",
      "- **Columns with Null Values:** 3\n",
      "  - **COMPANY:** 6,927 nulls (18.32%)\n",
      "  - **PHONE:** 5,794 nulls (15.32%)\n",
      "  - **ADDRESS:** 3,130 nulls (8.28%)\n",
      "\n",
      "### Duplicate Records Analysis:\n",
      "- **Duplicate Rows:** 0 (No duplicate records found)\n",
      "\n",
      "The table is free of duplicate records, but there are some columns with significant null values, particularly in `COMPANY`, `PHONE`, and `ADDRESS`.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "======================================================================\n",
      "RESULT\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_null_4 = run_smart_dq_check(\"Analyze missing data And duplicate records in PROD CUSTOMERS table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34dba101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent Output:\n",
      "The duplicate check for the table `PROD_SALES.DATA_MART_MARKETING.CUSTOMERS` in Snowflake has been completed successfully. No duplicate rows were found in the dataset.\n",
      "The duplicate check for the table `stage_sales.public.customers` in PostgreSQL has been completed. There are **1134 duplicate rows** detected in the dataset.\n",
      "Agent result_null_1:\n",
      "The null values analysis for the `stage_sales.public.customers` table in PostgreSQL is as follows:\n",
      "\n",
      "- **Total Rows:** 50,000\n",
      "- **Total Columns:** 22\n",
      "- **Columns with Null Values:** 4\n",
      "\n",
      "### Null Analysis:\n",
      "1. **Company**\n",
      "   - Null Count: 9,000\n",
      "   - Null Percentage: 18.0%\n",
      "2. **Phone**\n",
      "   - Null Count: 7,500\n",
      "   - Null Percentage: 15.0%\n",
      "3. **Email**\n",
      "   - Null Count: 6,000\n",
      "   - Null Percentage: 12.0%\n",
      "4. **Address**\n",
      "   - Null Count: 4,000\n",
      "   - Null Percentage: 8.0%\n",
      "\n",
      "Let me know if you need further assistance!\n",
      "Agent result_null_2:\n",
      "The missing data analysis for the table `PROD_SALES.DATA_MART_MARKETING.CUSTOMERS` in Snowflake has been completed. Here are the results:\n",
      "\n",
      "- **Total Rows:** 37,811\n",
      "- **Total Columns:** 22\n",
      "- **Columns with Null Values:** 3\n",
      "\n",
      "### Null Analysis:\n",
      "1. **COMPANY**\n",
      "   - Null Count: 6,927\n",
      "   - Null Percentage: 18.32%\n",
      "2. **PHONE**\n",
      "   - Null Count: 5,794\n",
      "   - Null Percentage: 15.32%\n",
      "3. **ADDRESS**\n",
      "   - Null Count: 3,130\n",
      "   - Null Percentage: 8.28%\n",
      "\n",
      "Let me know if you need further insights or actions on this data!\n",
      "Agent result_null_3:\n",
      "The analysis of missing data in the `stage_sales.public.customers` table (PostgreSQL) has been completed. Here are the results:\n",
      "\n",
      "- **Total Rows:** 50,000\n",
      "- **Total Columns:** 22\n",
      "- **Columns with Null Values:** 4\n",
      "\n",
      "### Null Analysis:\n",
      "1. **Company**\n",
      "   - Null Count: 9,000\n",
      "   - Null Percentage: 18.0%\n",
      "2. **Phone**\n",
      "   - Null Count: 7,500\n",
      "   - Null Percentage: 15.0%\n",
      "3. **Email**\n",
      "   - Null Count: 6,000\n",
      "   - Null Percentage: 12.0%\n",
      "4. **Address**\n",
      "   - Null Count: 4,000\n",
      "   - Null Percentage: 8.0%\n",
      "\n",
      "Let me know if you need further assistance or additional checks!\n"
     ]
    }
   ],
   "source": [
    "print(\"Agent Output:\")\n",
    "print(result_1['output'])\n",
    "print(result_2['output'])\n",
    "print(\"Agent result_null_1:\")\n",
    "print(result_null_1['output'])\n",
    "print(\"Agent result_null_2:\")\n",
    "print(result_null_2['output'])\n",
    "print(\"Agent result_null_3:\")\n",
    "print(result_null_3['output'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-dq-agent-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
