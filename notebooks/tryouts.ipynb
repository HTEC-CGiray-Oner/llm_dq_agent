{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcf7ad65",
   "metadata": {},
   "source": [
    "# Setup: Build Schema Index\n",
    "\n",
    "First, we need to index the table schemas from our databases so the agent can discover them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "596a0e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\CihanGirayOner\\Desktop\\git_repos\\llm_dq_agent\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from src.retrieval.schema_indexer import SchemaIndexer\n",
    "from src.agent.smart_planner import run_smart_dq_check\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import src.reporting.report_templates\n",
    "import src.data_quality.checks\n",
    "import src.reporting.report_generator\n",
    "import src.agent.reporting_tools\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "from src.reporting.report_templates import ReportTemplates\n",
    "from src.reporting.report_generator import DataQualityReportGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d58ad1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: c96a4de3-3bd8-45f4-9005-bbdbc704282d)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Connected to Snowflake: PROD_SALES.PUBLIC\n",
      "âœ“ Disconnected from Snowflake\n",
      "Auto-discovered 4 schemas: DATA_MART_FINANCE, DATA_MART_MANUFACTURING, DATA_MART_MARKETING, PUBLIC\n",
      "\n",
      "======================================================================\n",
      "[1/4] INDEXING SCHEMA: DATA_MART_FINANCE\n",
      "======================================================================\n",
      "âœ“ Disconnected from Snowflake\n",
      "Auto-discovered 4 schemas: DATA_MART_FINANCE, DATA_MART_MANUFACTURING, DATA_MART_MARKETING, PUBLIC\n",
      "\n",
      "======================================================================\n",
      "[1/4] INDEXING SCHEMA: DATA_MART_FINANCE\n",
      "======================================================================\n",
      "âœ“ Connected to Snowflake: PROD_SALES.PUBLIC\n",
      "Discovering tables in PROD_SALES.DATA_MART_FINANCE...\n",
      "âœ“ Connected to Snowflake: PROD_SALES.PUBLIC\n",
      "Discovering tables in PROD_SALES.DATA_MART_FINANCE...\n",
      "âœ“ Found 1 tables/views\n",
      "âœ“ Disconnected from Snowflake\n",
      "\n",
      "Generating metadata documents for 1 tables...\n",
      "  [1/1] Processing INVOICES...\n",
      "âœ“ Found 1 tables/views\n",
      "âœ“ Disconnected from Snowflake\n",
      "\n",
      "Generating metadata documents for 1 tables...\n",
      "  [1/1] Processing INVOICES...\n",
      "âœ“ Connected to Snowflake: PROD_SALES.PUBLIC\n",
      "Discovering tables in PROD_SALES.DATA_MART_FINANCE...\n",
      "âœ“ Connected to Snowflake: PROD_SALES.PUBLIC\n",
      "Discovering tables in PROD_SALES.DATA_MART_FINANCE...\n",
      "âœ“ Found 1 tables/views\n",
      "âœ“ Disconnected from Snowflake\n",
      "âœ“ Found 1 tables/views\n",
      "âœ“ Disconnected from Snowflake\n",
      "âœ“ Connected to Snowflake: PROD_SALES.PUBLIC\n",
      "âœ“ Connected to Snowflake: PROD_SALES.PUBLIC\n",
      "âœ“ Disconnected from Snowflake\n",
      "âœ“ Disconnected from Snowflake\n",
      "âœ“ Connected to Snowflake: PROD_SALES.PUBLIC\n",
      "âœ“ Connected to Snowflake: PROD_SALES.PUBLIC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Disconnected from Snowflake\n",
      "âœ“ Generated 1 metadata documents\n",
      "âœ“ Deleted existing collection: database_schemas\n",
      "\n",
      "Indexing 1 table metadata documents...\n",
      "âœ“ Indexed 1 tables from schema DATA_MART_FINANCE\n",
      "\n",
      "======================================================================\n",
      "[2/4] INDEXING SCHEMA: DATA_MART_MANUFACTURING\n",
      "======================================================================\n",
      "âœ“ Indexed 1 tables from schema DATA_MART_FINANCE\n",
      "\n",
      "======================================================================\n",
      "[2/4] INDEXING SCHEMA: DATA_MART_MANUFACTURING\n",
      "======================================================================\n",
      "âœ“ Connected to Snowflake: PROD_SALES.PUBLIC\n",
      "Discovering tables in PROD_SALES.DATA_MART_MANUFACTURING...\n",
      "âœ“ Connected to Snowflake: PROD_SALES.PUBLIC\n",
      "Discovering tables in PROD_SALES.DATA_MART_MANUFACTURING...\n",
      "âœ“ Found 1 tables/views\n",
      "âœ“ Disconnected from Snowflake\n",
      "\n",
      "Generating metadata documents for 1 tables...\n",
      "  [1/1] Processing PRODUCTS...\n",
      "âœ“ Found 1 tables/views\n",
      "âœ“ Disconnected from Snowflake\n",
      "\n",
      "Generating metadata documents for 1 tables...\n",
      "  [1/1] Processing PRODUCTS...\n",
      "âœ“ Connected to Snowflake: PROD_SALES.PUBLIC\n",
      "Discovering tables in PROD_SALES.DATA_MART_MANUFACTURING...\n",
      "âœ“ Connected to Snowflake: PROD_SALES.PUBLIC\n",
      "Discovering tables in PROD_SALES.DATA_MART_MANUFACTURING...\n",
      "âœ“ Found 1 tables/views\n",
      "âœ“ Disconnected from Snowflake\n",
      "âœ“ Found 1 tables/views\n",
      "âœ“ Disconnected from Snowflake\n",
      "âœ“ Connected to Snowflake: PROD_SALES.PUBLIC\n",
      "âœ“ Connected to Snowflake: PROD_SALES.PUBLIC\n",
      "âœ“ Disconnected from Snowflake\n",
      "âœ“ Disconnected from Snowflake\n",
      "âœ“ Connected to Snowflake: PROD_SALES.PUBLIC\n",
      "âœ“ Connected to Snowflake: PROD_SALES.PUBLIC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Disconnected from Snowflake\n",
      "âœ“ Generated 1 metadata documents\n",
      "\n",
      "Indexing 1 table metadata documents...\n",
      "âœ“ Indexed 1 tables from schema DATA_MART_MANUFACTURING\n",
      "\n",
      "======================================================================\n",
      "[3/4] INDEXING SCHEMA: DATA_MART_MARKETING\n",
      "======================================================================\n",
      "âœ“ Connected to Snowflake: PROD_SALES.PUBLIC\n",
      "Discovering tables in PROD_SALES.DATA_MART_MARKETING...\n",
      "âœ“ Connected to Snowflake: PROD_SALES.PUBLIC\n",
      "Discovering tables in PROD_SALES.DATA_MART_MARKETING...\n",
      "âœ“ Found 1 tables/views\n",
      "âœ“ Disconnected from Snowflake\n",
      "\n",
      "Generating metadata documents for 1 tables...\n",
      "  [1/1] Processing CUSTOMERS...\n",
      "âœ“ Found 1 tables/views\n",
      "âœ“ Disconnected from Snowflake\n",
      "\n",
      "Generating metadata documents for 1 tables...\n",
      "  [1/1] Processing CUSTOMERS...\n",
      "âœ“ Connected to Snowflake: PROD_SALES.PUBLIC\n",
      "Discovering tables in PROD_SALES.DATA_MART_MARKETING...\n",
      "âœ“ Connected to Snowflake: PROD_SALES.PUBLIC\n",
      "Discovering tables in PROD_SALES.DATA_MART_MARKETING...\n",
      "âœ“ Found 1 tables/views\n",
      "âœ“ Disconnected from Snowflake\n",
      "âœ“ Found 1 tables/views\n",
      "âœ“ Disconnected from Snowflake\n",
      "âœ“ Connected to Snowflake: PROD_SALES.PUBLIC\n",
      "âœ“ Connected to Snowflake: PROD_SALES.PUBLIC\n",
      "âœ“ Disconnected from Snowflake\n",
      "âœ“ Disconnected from Snowflake\n",
      "âœ“ Connected to Snowflake: PROD_SALES.PUBLIC\n",
      "âœ“ Connected to Snowflake: PROD_SALES.PUBLIC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Disconnected from Snowflake\n",
      "âœ“ Generated 1 metadata documents\n",
      "\n",
      "Indexing 1 table metadata documents...\n",
      "âœ“ Indexed 1 tables from schema DATA_MART_MARKETING\n",
      "\n",
      "======================================================================\n",
      "[4/4] INDEXING SCHEMA: PUBLIC\n",
      "======================================================================\n",
      "âœ“ Connected to Snowflake: PROD_SALES.PUBLIC\n",
      "Discovering tables in PROD_SALES.PUBLIC...\n",
      "âœ“ Connected to Snowflake: PROD_SALES.PUBLIC\n",
      "Discovering tables in PROD_SALES.PUBLIC...\n",
      "âœ“ Found 0 tables/views\n",
      "âœ“ Disconnected from Snowflake\n",
      "\n",
      "Generating metadata documents for 0 tables...\n",
      "âœ“ Generated 0 metadata documents\n",
      "No tables found in schema PUBLIC\n",
      "\n",
      "======================================================================\n",
      "âœ“ SCHEMA INDEX BUILT SUCCESSFULLY\n",
      "  - Collection: database_schemas\n",
      "  - Schemas indexed: 4\n",
      "  - Total tables indexed: 3\n",
      "  - Path: ./chroma_db\n",
      "======================================================================\n",
      "\n",
      "âœ“ Found 0 tables/views\n",
      "âœ“ Disconnected from Snowflake\n",
      "\n",
      "Generating metadata documents for 0 tables...\n",
      "âœ“ Generated 0 metadata documents\n",
      "No tables found in schema PUBLIC\n",
      "\n",
      "======================================================================\n",
      "âœ“ SCHEMA INDEX BUILT SUCCESSFULLY\n",
      "  - Collection: database_schemas\n",
      "  - Schemas indexed: 4\n",
      "  - Total tables indexed: 3\n",
      "  - Path: ./chroma_db\n",
      "======================================================================\n",
      "\n",
      "âœ“ Connected to PostgreSQL: stage_sales\n",
      "âœ“ Disconnected from PostgreSQL\n",
      "Auto-discovered 1 schemas: public\n",
      "\n",
      "======================================================================\n",
      "[1/1] INDEXING SCHEMA: public\n",
      "======================================================================\n",
      "âœ“ Connected to PostgreSQL: stage_sales\n",
      "Discovering tables in stage_sales.public...\n",
      "âœ“ Found 3 tables/views\n",
      "âœ“ Disconnected from PostgreSQL\n",
      "\n",
      "Generating metadata documents for 3 tables...\n",
      "  [1/3] Processing customers...\n",
      "âœ“ Connected to PostgreSQL: stage_sales\n",
      "âœ“ Disconnected from PostgreSQL\n",
      "Auto-discovered 1 schemas: public\n",
      "\n",
      "======================================================================\n",
      "[1/1] INDEXING SCHEMA: public\n",
      "======================================================================\n",
      "âœ“ Connected to PostgreSQL: stage_sales\n",
      "Discovering tables in stage_sales.public...\n",
      "âœ“ Found 3 tables/views\n",
      "âœ“ Disconnected from PostgreSQL\n",
      "\n",
      "Generating metadata documents for 3 tables...\n",
      "  [1/3] Processing customers...\n",
      "âœ“ Connected to PostgreSQL: stage_sales\n",
      "Discovering tables in stage_sales.public...\n",
      "âœ“ Found 3 tables/views\n",
      "âœ“ Disconnected from PostgreSQL\n",
      "âœ“ Connected to PostgreSQL: stage_sales\n",
      "âœ“ Connected to PostgreSQL: stage_sales\n",
      "Discovering tables in stage_sales.public...\n",
      "âœ“ Found 3 tables/views\n",
      "âœ“ Disconnected from PostgreSQL\n",
      "âœ“ Connected to PostgreSQL: stage_sales\n",
      "âœ“ Disconnected from PostgreSQL\n",
      "âœ“ Connected to PostgreSQL: stage_sales\n",
      "âœ“ Disconnected from PostgreSQL\n",
      "  [2/3] Processing invoices...\n",
      "âœ“ Disconnected from PostgreSQL\n",
      "âœ“ Connected to PostgreSQL: stage_sales\n",
      "âœ“ Disconnected from PostgreSQL\n",
      "  [2/3] Processing invoices...\n",
      "âœ“ Connected to PostgreSQL: stage_sales\n",
      "Discovering tables in stage_sales.public...\n",
      "âœ“ Found 3 tables/views\n",
      "âœ“ Disconnected from PostgreSQL\n",
      "âœ“ Connected to PostgreSQL: stage_sales\n",
      "âœ“ Disconnected from PostgreSQL\n",
      "âœ“ Connected to PostgreSQL: stage_sales\n",
      "Discovering tables in stage_sales.public...\n",
      "âœ“ Found 3 tables/views\n",
      "âœ“ Disconnected from PostgreSQL\n",
      "âœ“ Connected to PostgreSQL: stage_sales\n",
      "âœ“ Disconnected from PostgreSQL\n",
      "âœ“ Connected to PostgreSQL: stage_sales\n",
      "âœ“ Disconnected from PostgreSQL\n",
      "  [3/3] Processing products...\n",
      "âœ“ Connected to PostgreSQL: stage_sales\n",
      "Discovering tables in stage_sales.public...\n",
      "âœ“ Found 3 tables/views\n",
      "âœ“ Disconnected from PostgreSQL\n",
      "âœ“ Connected to PostgreSQL: stage_sales\n",
      "âœ“ Disconnected from PostgreSQL\n",
      "  [3/3] Processing products...\n",
      "âœ“ Connected to PostgreSQL: stage_sales\n",
      "Discovering tables in stage_sales.public...\n",
      "âœ“ Found 3 tables/views\n",
      "âœ“ Disconnected from PostgreSQL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Connected to PostgreSQL: stage_sales\n",
      "âœ“ Disconnected from PostgreSQL\n",
      "âœ“ Connected to PostgreSQL: stage_sales\n",
      "âœ“ Disconnected from PostgreSQL\n",
      "âœ“ Generated 3 metadata documents\n",
      "\n",
      "Indexing 3 table metadata documents...\n",
      "âœ“ Indexed 3 tables from schema public\n",
      "\n",
      "======================================================================\n",
      "âœ“ SCHEMA INDEX BUILT SUCCESSFULLY\n",
      "  - Collection: database_schemas\n",
      "  - Schemas indexed: 1\n",
      "  - Total tables indexed: 3\n",
      "  - Path: ./chroma_db\n",
      "======================================================================\n",
      "\n",
      "âœ“ Indexed 3 tables from schema public\n",
      "\n",
      "======================================================================\n",
      "âœ“ SCHEMA INDEX BUILT SUCCESSFULLY\n",
      "  - Collection: database_schemas\n",
      "  - Schemas indexed: 1\n",
      "  - Total tables indexed: 3\n",
      "  - Path: ./chroma_db\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Index Snowflake first (recreate the collection)\n",
    "snowflake_indexer = SchemaIndexer(connector_type='snowflake')\n",
    "snowflake_indexer.build_schema_index(recreate=True)\n",
    "\n",
    "# Index PostgreSQL second (append to collection)\n",
    "postgres_indexer = SchemaIndexer(connector_type='postgres')\n",
    "postgres_indexer.build_schema_index(recreate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bb9e9e",
   "metadata": {},
   "source": [
    "## ğŸ“Š Enhanced Comprehensive Report Creation\n",
    "\n",
    "Generate comprehensive data quality reports with detailed duplicate counts, null analysis, and descriptive statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c65981e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SMART DATA QUALITY CHECK\n",
      "======================================================================\n",
      "User Query: Generate comprehensive DQ report for staging invoices table\n",
      "\n",
      "Step 1: Discovering relevant tables...\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event CollectionGetEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event CollectionGetEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event CollectionQueryEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event CollectionQueryEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 3 relevant table(s):\n",
      "\n",
      "  1. [POSTGRES] stage_sales.public.invoices\n",
      "     Relevance: 10.35%\n",
      "\n",
      "  2. [SNOWFLAKE] PROD_SALES.DATA_MART_FINANCE.INVOICES\n",
      "     Relevance: 6.72%\n",
      "\n",
      "  3. [SNOWFLAKE] PROD_SALES.DATA_MART_MANUFACTURING.PRODUCTS\n",
      "     Relevance: -32.84%\n",
      "\n",
      "Step 2: Building context for agent...\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Step 3: Running agent with discovered tables...\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `generate_comprehensive_dq_report` with `{'dataset_id': 'stage_sales.public.invoices', 'connector_type': 'postgres', 'output_format': 'markdown'}`\n",
      "\n",
      "\n",
      "\u001b[0mğŸ” Running duplicates check...\n",
      "--- Loading data for: stage_sales.public.invoices using POSTGRES connector ---\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `generate_comprehensive_dq_report` with `{'dataset_id': 'stage_sales.public.invoices', 'connector_type': 'postgres', 'output_format': 'markdown'}`\n",
      "\n",
      "\n",
      "\u001b[0mğŸ” Running duplicates check...\n",
      "--- Loading data for: stage_sales.public.invoices using POSTGRES connector ---\n",
      "âœ“ Connected to PostgreSQL: stage_sales\n",
      "Executing query: SELECT * FROM stage_sales.public.invoices\n",
      "âœ“ Connected to PostgreSQL: stage_sales\n",
      "Executing query: SELECT * FROM stage_sales.public.invoices\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CihanGirayOner\\Desktop\\git_repos\\llm_dq_agent\\src\\connectors\\postgres_connector.py:78: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(sql_query, self._connection)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Loaded 100000 rows from PostgreSQL\n",
      "âœ“ Disconnected from PostgreSQL\n",
      "ğŸ” Running null_values check...\n",
      "--- Loading data for: stage_sales.public.invoices using POSTGRES connector ---\n",
      "âœ“ Connected to PostgreSQL: stage_sales\n",
      "Executing query: SELECT * FROM stage_sales.public.invoices\n",
      "âœ“ Connected to PostgreSQL: stage_sales\n",
      "Executing query: SELECT * FROM stage_sales.public.invoices\n",
      "âœ“ Loaded 100000 rows from PostgreSQL\n",
      "âœ“ Disconnected from PostgreSQL\n",
      "âœ“ Loaded 100000 rows from PostgreSQL\n",
      "âœ“ Disconnected from PostgreSQL\n",
      "ğŸ” Running descriptive_stats check...\n",
      "--- Loading data for: stage_sales.public.invoices using POSTGRES connector ---\n",
      "âœ“ Connected to PostgreSQL: stage_sales\n",
      "Executing query: SELECT * FROM stage_sales.public.invoices\n",
      "ğŸ” Running descriptive_stats check...\n",
      "--- Loading data for: stage_sales.public.invoices using POSTGRES connector ---\n",
      "âœ“ Connected to PostgreSQL: stage_sales\n",
      "Executing query: SELECT * FROM stage_sales.public.invoices\n",
      "âœ“ Loaded 100000 rows from PostgreSQL\n",
      "âœ“ Disconnected from PostgreSQL\n",
      "âœ“ Converted 'invoice_id' to categorical for descriptive statistics\n",
      "âœ“ Converted 'customer_id' to categorical for descriptive statistics\n",
      "âœ“ Converted 'product_id' to categorical for descriptive statistics\n",
      "âœ“ Loaded 100000 rows from PostgreSQL\n",
      "âœ“ Disconnected from PostgreSQL\n",
      "âœ“ Converted 'invoice_id' to categorical for descriptive statistics\n",
      "âœ“ Converted 'customer_id' to categorical for descriptive statistics\n",
      "âœ“ Converted 'product_id' to categorical for descriptive statistics\n",
      "\u001b[36;1m\u001b[1;3m{'dataset_id': 'stage_sales.public.invoices', 'connector_type': 'postgres', 'report_format': 'markdown', 'report_content': '# Data Quality Assessment Report\\n\\n## Dataset Information\\n- **Dataset**: `stage_sales.public.invoices`\\n- **Connector**: POSTGRES\\n- **Assessment Date**: 2025-11-20 13:55:16\\n- **Total Checks**: 3\\n\\n## Executive Summary\\n- âœ… **Passed**: 2 checks\\n- âŒ **Failed**: 1 checks\\n- ğŸš« **Errors**: 0 checks\\n\\n---\\n\\n## Detailed Check Results\\n\\n### âœ… Duplicates\\n\\n- **Status**: PASS\\n- **Total Rows**: 100,000\\n- **Duplicate Records**: 0 (0.00% of data)\\n- **Quality**: Excellent - No duplicate records found\\n\\n### âœ… Null Values\\n\\n- **Status**: FAIL\\n- **Dataset Size**: 100,000 rows Ã— 20 columns\\n- **Columns with Missing Data**: 9 out of 20 columns\\n- **Missing Data Coverage**: 45.0% of columns affected\\n\\n**Detailed Null Analysis:**\\n- `notes`: 74,976 nulls (75.0%)\\n- `discount_amount`: 64,948 nulls (65.0%)\\n- `customer_company`: 19,953 nulls (19.9%)\\n- `shipping_address`: 17,997 nulls (18.0%)\\n- `billing_address`: 14,893 nulls (14.9%)\\n- `due_date`: 11,894 nulls (11.9%)\\n- `payment_method`: 7,859 nulls (7.9%)\\n- `tax_rate`: 5,141 nulls (5.1%)\\n- `tax_amount`: 5,141 nulls (5.1%)\\n\\n### âœ… Descriptive Stats\\n\\n- **Status**: PASS (Statistics Generated)\\n- **Columns Analyzed**: 20 columns\\n- **Numerical Columns**: 20 (invoice_id, customer_id, product_id, invoice_date, due_date...)\\n- **Categorical Columns**: 0 ()\\n\\n**Key Numerical Statistics:**\\n- `invoice_id`: Mean=None, Std=None, Range=[None to None]\\n- `customer_id`: Mean=None, Std=None, Range=[None to None]\\n- `product_id`: Mean=None, Std=None, Range=[None to None]\\n\\n---\\n\\n## ğŸ¯ Recommendations\\n\\n1. **Address High Null Value Columns**\\n   - Columns `notes`, `discount_amount` have >20% null values. Consider: 1) Data source validation, 2) Default value strategies, 3) Imputation techniques, or 4) Column removal if not critical.\\n   - *Priority*: High\\n\\n2. **Implement Null Handling Strategy**\\n   - Columns `customer_company`, `shipping_address`, `billing_address` have 5-20% null values. Consider implementing NOT NULL constraints, data validation rules, or imputation strategies based on business logic.\\n   - *Priority*: Medium\\n\\n3. **Comprehensive Data Governance**\\n   - 9 columns have null values. Implement data governance policies including mandatory field validation, data entry training, and upstream data quality monitoring.\\n   - *Priority*: Medium\\n\\n4. **Enhance Data Quality Processes**\\n   - 67% pass rate indicates room for improvement. Implement automated quality gates, exception handling, and proactive monitoring for sustained quality.\\n   - *Priority*: Medium\\n\\n5. **High Cardinality in invoice_id**\\n   - Column `invoice_id` has very high cardinality (100,000 unique values). Consider data normalization, categorization, or indexing strategies for performance.\\n   - *Priority*: Low\\n\\n6. **Investigate High Variability in quantity**\\n   - Column `quantity` has high variability (CV=7.92). Review for outliers, data entry errors, or consider data transformation/normalization techniques.\\n   - *Priority*: Low\\n\\n7. **Investigate High Variability in unit_price**\\n   - Column `unit_price` has high variability (CV=6.97). Review for outliers, data entry errors, or consider data transformation/normalization techniques.\\n   - *Priority*: Low\\n\\n8. **Investigate High Variability in subtotal**\\n   - Column `subtotal` has high variability (CV=53.26). Review for outliers, data entry errors, or consider data transformation/normalization techniques.\\n   - *Priority*: Low\\n\\n9. **Investigate High Variability in discount_amount**\\n   - Column `discount_amount` has high variability (CV=66.99). Review for outliers, data entry errors, or consider data transformation/normalization techniques.\\n   - *Priority*: Low\\n\\n10. **Investigate High Variability in tax_rate**\\n   - Column `tax_rate` has high variability (CV=8.28). Review for outliers, data entry errors, or consider data transformation/normalization techniques.\\n   - *Priority*: Low\\n\\n11. **Investigate High Variability in tax_amount**\\n   - Column `tax_amount` has high variability (CV=1102.89). Review for outliers, data entry errors, or consider data transformation/normalization techniques.\\n   - *Priority*: Low\\n\\n12. **Investigate High Variability in total_amount**\\n   - Column `total_amount` has high variability (CV=131.54). Review for outliers, data entry errors, or consider data transformation/normalization techniques.\\n   - *Priority*: Low\\n\\n13. **Data Documentation and Profiling**\\n   - Dataset has 20 columns. Consider creating data dictionaries, establishing column-level monitoring, and implementing regular statistical profiling to track data drift over time.\\n   - *Priority*: Low\\n\\n---\\n\\n## Report Generation\\n- **Generated by**: LLM Data Quality Agent\\n- **Generation Time**: 2025-11-20 13:55:21\\n- **Report Format**: Markdown\\n\\n*This report was automatically generated. Please review recommendations and take appropriate action.*\\n', 'summary': {'total_checks': 3, 'passed_checks': 2, 'failed_checks': 1, 'error_checks': 0, 'pass_rate': '66.7%'}, 'recommendations_count': 13, 'status': 'success'}\u001b[0m\u001b[36;1m\u001b[1;3m{'dataset_id': 'stage_sales.public.invoices', 'connector_type': 'postgres', 'report_format': 'markdown', 'report_content': '# Data Quality Assessment Report\\n\\n## Dataset Information\\n- **Dataset**: `stage_sales.public.invoices`\\n- **Connector**: POSTGRES\\n- **Assessment Date**: 2025-11-20 13:55:16\\n- **Total Checks**: 3\\n\\n## Executive Summary\\n- âœ… **Passed**: 2 checks\\n- âŒ **Failed**: 1 checks\\n- ğŸš« **Errors**: 0 checks\\n\\n---\\n\\n## Detailed Check Results\\n\\n### âœ… Duplicates\\n\\n- **Status**: PASS\\n- **Total Rows**: 100,000\\n- **Duplicate Records**: 0 (0.00% of data)\\n- **Quality**: Excellent - No duplicate records found\\n\\n### âœ… Null Values\\n\\n- **Status**: FAIL\\n- **Dataset Size**: 100,000 rows Ã— 20 columns\\n- **Columns with Missing Data**: 9 out of 20 columns\\n- **Missing Data Coverage**: 45.0% of columns affected\\n\\n**Detailed Null Analysis:**\\n- `notes`: 74,976 nulls (75.0%)\\n- `discount_amount`: 64,948 nulls (65.0%)\\n- `customer_company`: 19,953 nulls (19.9%)\\n- `shipping_address`: 17,997 nulls (18.0%)\\n- `billing_address`: 14,893 nulls (14.9%)\\n- `due_date`: 11,894 nulls (11.9%)\\n- `payment_method`: 7,859 nulls (7.9%)\\n- `tax_rate`: 5,141 nulls (5.1%)\\n- `tax_amount`: 5,141 nulls (5.1%)\\n\\n### âœ… Descriptive Stats\\n\\n- **Status**: PASS (Statistics Generated)\\n- **Columns Analyzed**: 20 columns\\n- **Numerical Columns**: 20 (invoice_id, customer_id, product_id, invoice_date, due_date...)\\n- **Categorical Columns**: 0 ()\\n\\n**Key Numerical Statistics:**\\n- `invoice_id`: Mean=None, Std=None, Range=[None to None]\\n- `customer_id`: Mean=None, Std=None, Range=[None to None]\\n- `product_id`: Mean=None, Std=None, Range=[None to None]\\n\\n---\\n\\n## ğŸ¯ Recommendations\\n\\n1. **Address High Null Value Columns**\\n   - Columns `notes`, `discount_amount` have >20% null values. Consider: 1) Data source validation, 2) Default value strategies, 3) Imputation techniques, or 4) Column removal if not critical.\\n   - *Priority*: High\\n\\n2. **Implement Null Handling Strategy**\\n   - Columns `customer_company`, `shipping_address`, `billing_address` have 5-20% null values. Consider implementing NOT NULL constraints, data validation rules, or imputation strategies based on business logic.\\n   - *Priority*: Medium\\n\\n3. **Comprehensive Data Governance**\\n   - 9 columns have null values. Implement data governance policies including mandatory field validation, data entry training, and upstream data quality monitoring.\\n   - *Priority*: Medium\\n\\n4. **Enhance Data Quality Processes**\\n   - 67% pass rate indicates room for improvement. Implement automated quality gates, exception handling, and proactive monitoring for sustained quality.\\n   - *Priority*: Medium\\n\\n5. **High Cardinality in invoice_id**\\n   - Column `invoice_id` has very high cardinality (100,000 unique values). Consider data normalization, categorization, or indexing strategies for performance.\\n   - *Priority*: Low\\n\\n6. **Investigate High Variability in quantity**\\n   - Column `quantity` has high variability (CV=7.92). Review for outliers, data entry errors, or consider data transformation/normalization techniques.\\n   - *Priority*: Low\\n\\n7. **Investigate High Variability in unit_price**\\n   - Column `unit_price` has high variability (CV=6.97). Review for outliers, data entry errors, or consider data transformation/normalization techniques.\\n   - *Priority*: Low\\n\\n8. **Investigate High Variability in subtotal**\\n   - Column `subtotal` has high variability (CV=53.26). Review for outliers, data entry errors, or consider data transformation/normalization techniques.\\n   - *Priority*: Low\\n\\n9. **Investigate High Variability in discount_amount**\\n   - Column `discount_amount` has high variability (CV=66.99). Review for outliers, data entry errors, or consider data transformation/normalization techniques.\\n   - *Priority*: Low\\n\\n10. **Investigate High Variability in tax_rate**\\n   - Column `tax_rate` has high variability (CV=8.28). Review for outliers, data entry errors, or consider data transformation/normalization techniques.\\n   - *Priority*: Low\\n\\n11. **Investigate High Variability in tax_amount**\\n   - Column `tax_amount` has high variability (CV=1102.89). Review for outliers, data entry errors, or consider data transformation/normalization techniques.\\n   - *Priority*: Low\\n\\n12. **Investigate High Variability in total_amount**\\n   - Column `total_amount` has high variability (CV=131.54). Review for outliers, data entry errors, or consider data transformation/normalization techniques.\\n   - *Priority*: Low\\n\\n13. **Data Documentation and Profiling**\\n   - Dataset has 20 columns. Consider creating data dictionaries, establishing column-level monitoring, and implementing regular statistical profiling to track data drift over time.\\n   - *Priority*: Low\\n\\n---\\n\\n## Report Generation\\n- **Generated by**: LLM Data Quality Agent\\n- **Generation Time**: 2025-11-20 13:55:21\\n- **Report Format**: Markdown\\n\\n*This report was automatically generated. Please review recommendations and take appropriate action.*\\n', 'summary': {'total_checks': 3, 'passed_checks': 2, 'failed_checks': 1, 'error_checks': 0, 'pass_rate': '66.7%'}, 'recommendations_count': 13, 'status': 'success'}\u001b[0m\u001b[32;1m\u001b[1;3mThe comprehensive data quality report for the `stage_sales.public.invoices` table has been successfully generated. Below is the summary:\n",
      "\n",
      "### Executive Summary\n",
      "- **Total Checks**: 3\n",
      "- **Passed**: 2 checks\n",
      "- **Failed**: 1 check\n",
      "- **Pass Rate**: 66.7%\n",
      "\n",
      "### Key Findings\n",
      "1. **Duplicates**: No duplicate records found (Excellent quality).\n",
      "2. **Null Values**: 9 out of 20 columns have missing data, with `notes` and `discount_amount` having the highest null percentages (75% and 65%, respectively).\n",
      "3. **Descriptive Stats**: Statistics generated successfully for all columns.\n",
      "\n",
      "### Recommendations\n",
      "- Address high null value columns (`notes`, `discount_amount`) with imputation or validation strategies.\n",
      "- Implement data governance policies to reduce null values across columns.\n",
      "- Investigate high variability in numerical columns like `quantity`, `unit_price`, and `total_amount` for potential outliers or errors.\n",
      "\n",
      "The full report is available in Markdown format. Let me know if you'd like to save or export this report in other formats!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "======================================================================\n",
      "RESULT\n",
      "======================================================================\n",
      "\n",
      "\u001b[32;1m\u001b[1;3mThe comprehensive data quality report for the `stage_sales.public.invoices` table has been successfully generated. Below is the summary:\n",
      "\n",
      "### Executive Summary\n",
      "- **Total Checks**: 3\n",
      "- **Passed**: 2 checks\n",
      "- **Failed**: 1 check\n",
      "- **Pass Rate**: 66.7%\n",
      "\n",
      "### Key Findings\n",
      "1. **Duplicates**: No duplicate records found (Excellent quality).\n",
      "2. **Null Values**: 9 out of 20 columns have missing data, with `notes` and `discount_amount` having the highest null percentages (75% and 65%, respectively).\n",
      "3. **Descriptive Stats**: Statistics generated successfully for all columns.\n",
      "\n",
      "### Recommendations\n",
      "- Address high null value columns (`notes`, `discount_amount`) with imputation or validation strategies.\n",
      "- Implement data governance policies to reduce null values across columns.\n",
      "- Investigate high variability in numerical columns like `quantity`, `unit_price`, and `total_amount` for potential outliers or errors.\n",
      "\n",
      "The full report is available in Markdown format. Let me know if you'd like to save or export this report in other formats!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "======================================================================\n",
      "RESULT\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate comprehensive report using the smart agent\n",
    "comprehensive_report = run_smart_dq_check(\n",
    "    \"Generate comprehensive DQ report for staging invoices table\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f053675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Report content preview: The comprehensive data quality report for the `stage_sales.public.invoices` table has been successfully generated. Below is the summary:\n",
      "\n",
      "### Executive Summary\n",
      "- **Total Checks**: 3\n",
      "- **Passed**: 2 ch...\n",
      "âœ… Extracted info:\n",
      "   ğŸ“Š Dataset: stage_sales.public.invoices\n",
      "   ğŸ”— Connector: postgres\n",
      "\n",
      "ğŸ” Running full DQ assessment to get ACTUAL results...\n",
      "ğŸ” Running duplicates check...\n",
      "--- Loading data for: stage_sales.public.invoices using POSTGRES connector ---\n",
      "âœ“ Connected to PostgreSQL: stage_sales\n",
      "Executing query: SELECT * FROM stage_sales.public.invoices\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CihanGirayOner\\Desktop\\git_repos\\llm_dq_agent\\src\\connectors\\postgres_connector.py:78: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(sql_query, self._connection)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Loaded 100000 rows from PostgreSQL\n",
      "âœ“ Disconnected from PostgreSQL\n",
      "ğŸ” Running null_values check...\n",
      "--- Loading data for: stage_sales.public.invoices using POSTGRES connector ---\n",
      "âœ“ Connected to PostgreSQL: stage_sales\n",
      "Executing query: SELECT * FROM stage_sales.public.invoices\n",
      "âœ“ Connected to PostgreSQL: stage_sales\n",
      "Executing query: SELECT * FROM stage_sales.public.invoices\n",
      "âœ“ Loaded 100000 rows from PostgreSQL\n",
      "âœ“ Disconnected from PostgreSQL\n",
      "âœ“ Loaded 100000 rows from PostgreSQL\n",
      "âœ“ Disconnected from PostgreSQL\n",
      "ğŸ” Running descriptive_stats check...\n",
      "--- Loading data for: stage_sales.public.invoices using POSTGRES connector ---\n",
      "âœ“ Connected to PostgreSQL: stage_sales\n",
      "Executing query: SELECT * FROM stage_sales.public.invoices\n",
      "ğŸ” Running descriptive_stats check...\n",
      "--- Loading data for: stage_sales.public.invoices using POSTGRES connector ---\n",
      "âœ“ Connected to PostgreSQL: stage_sales\n",
      "Executing query: SELECT * FROM stage_sales.public.invoices\n",
      "âœ“ Loaded 100000 rows from PostgreSQL\n",
      "âœ“ Disconnected from PostgreSQL\n",
      "âœ“ Converted 'invoice_id' to categorical for descriptive statistics\n",
      "âœ“ Converted 'customer_id' to categorical for descriptive statistics\n",
      "âœ“ Converted 'product_id' to categorical for descriptive statistics\n",
      "âœ“ Loaded 100000 rows from PostgreSQL\n",
      "âœ“ Disconnected from PostgreSQL\n",
      "âœ“ Converted 'invoice_id' to categorical for descriptive statistics\n",
      "âœ“ Converted 'customer_id' to categorical for descriptive statistics\n",
      "âœ“ Converted 'product_id' to categorical for descriptive statistics\n",
      "\n",
      "ğŸ“Š ACTUAL Data Quality Results for stage_sales.public.invoices:\n",
      "   âœ… Passed: 2 checks\n",
      "   âŒ Failed: 1 checks\n",
      "   ğŸš« Errors: 0 checks\n",
      "\n",
      "ğŸ“„ Generating reports using standardized templates...\n",
      "   âœ… MARKDOWN: ../reports/comprehensive_dq_report_20251120_135532.md\n",
      "\n",
      "ğŸ“Š ACTUAL Data Quality Results for stage_sales.public.invoices:\n",
      "   âœ… Passed: 2 checks\n",
      "   âŒ Failed: 1 checks\n",
      "   ğŸš« Errors: 0 checks\n",
      "\n",
      "ğŸ“„ Generating reports using standardized templates...\n",
      "   âœ… MARKDOWN: ../reports/comprehensive_dq_report_20251120_135532.md\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Create reports directory one folder up\n",
    "reports_dir = \"../reports\"\n",
    "os.makedirs(reports_dir, exist_ok=True)\n",
    "\n",
    "# Initialize report generator\n",
    "generator = DataQualityReportGenerator()\n",
    "\n",
    "# Generate timestamp for unique filenames\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Extract dataset and connector info from the comprehensive_report\n",
    "report_output = comprehensive_report.get('output', '')\n",
    "print(f\"ğŸ“ Report content preview: {report_output[:200]}...\")\n",
    "\n",
    "# Try to extract dataset_id and connector_type from the report output\n",
    "dataset_id = None\n",
    "connector_type = None\n",
    "\n",
    "# Look for patterns like \"stage_sales.public.customers\" or \"PROD_SALES.PUBLIC.CUSTOMERS\"\n",
    "dataset_patterns = [\n",
    "    r'`([^`]+\\.public\\.[^`]+)`',  # Pattern: `schema.public.table`\n",
    "    r'([A-Z_]+\\.[A-Z_]+\\.[A-Z_]+)',  # Pattern: SCHEMA.PUBLIC.TABLE\n",
    "    r'([a-z_]+\\.public\\.[a-z_]+)',   # Pattern: schema.public.table\n",
    "]\n",
    "\n",
    "for pattern in dataset_patterns:\n",
    "    match = re.search(pattern, report_output)\n",
    "    if match:\n",
    "        dataset_id = match.group(1)\n",
    "        break\n",
    "\n",
    "# Determine connector type based on dataset naming convention or explicit mention\n",
    "if dataset_id:\n",
    "    if 'PROD_SALES' in dataset_id.upper() or 'snowflake' in report_output.lower():\n",
    "        connector_type = 'snowflake'\n",
    "    elif 'STAGE_SALES' in dataset_id.upper() or 'postgres' in report_output.lower():\n",
    "        connector_type = 'postgres'\n",
    "    else:\n",
    "        # Default based on case - uppercase typically Snowflake, lowercase typically Postgres\n",
    "        connector_type = 'snowflake' if dataset_id.isupper() else 'postgres'\n",
    "\n",
    "# Fallback if extraction fails\n",
    "if not dataset_id:\n",
    "    print(\"âš ï¸ Could not extract dataset from report, using default...\")\n",
    "    dataset_id = \"stage_sales.public.customers\"\n",
    "    connector_type = \"postgres\"\n",
    "\n",
    "print(f\"âœ… Extracted info:\")\n",
    "print(f\"   ğŸ“Š Dataset: {dataset_id}\")\n",
    "print(f\"   ğŸ”— Connector: {connector_type}\")\n",
    "\n",
    "base_filename = f\"comprehensive_dq_report_{timestamp}\"\n",
    "\n",
    "print(f\"\\nğŸ” Running full DQ assessment to get ACTUAL results...\")\n",
    "\n",
    "# Run ACTUAL comprehensive assessment using extracted dataset info\n",
    "assessment_results = generator.run_full_assessment(\n",
    "    dataset_id=dataset_id,\n",
    "    connector_type=connector_type\n",
    ")\n",
    "\n",
    "# Display summary of actual results\n",
    "print(f\"\\nğŸ“Š ACTUAL Data Quality Results for {dataset_id}:\")\n",
    "print(f\"   âœ… Passed: {assessment_results['summary']['passed_checks']} checks\")\n",
    "print(f\"   âŒ Failed: {assessment_results['summary']['failed_checks']} checks\")\n",
    "print(f\"   ğŸš« Errors: {assessment_results['summary']['error_checks']} checks\")\n",
    "\n",
    "print(f\"\\nğŸ“„ Generating reports using standardized templates...\")\n",
    "\n",
    "# 1. Save as Markdown (.md) using template\n",
    "md_file = f\"{reports_dir}/{base_filename}.md\"\n",
    "md_content = generator.generate_markdown_report(assessment_results)\n",
    "with open(md_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(md_content)\n",
    "print(f\"   âœ… MARKDOWN: {md_file}\")\n",
    "\n",
    "# 2. Save as HTML (.html) using template\n",
    "html_file = f\"{reports_dir}/{base_filename}.html\"\n",
    "html_content = generator.generate_html_report(assessment_results)\n",
    "with open(html_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(html_content)\n",
    "\n",
    "# 3. Save as JSON (.json) using generator\n",
    "json_file = f\"{reports_dir}/{base_filename}.json\"\n",
    "json_content = generator.generate_json_report(assessment_results)\n",
    "with open(json_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(json_content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-dq-agent-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
